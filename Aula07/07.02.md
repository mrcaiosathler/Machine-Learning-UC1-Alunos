# Redes Neurais Recorrentes (RNN)

## 🎯 Objetivo da Aula

Nesta aula, vamos introduzir o conceito de **Redes Neurais Recorrentes (RNN)**, um tipo especial de rede neural projetada para **lidar com dados sequenciais**, como **textos e séries temporais**. Ao final da aula, você entenderá:

- Como as RNNs funcionam
- Por que elas são úteis para sequências
- Os principais desafios do uso das RNN tradicionais
- Soluções avançadas como **LSTM** (Long Short-Term Memory - Memória de Curto Prazo Longa) e **GRU** ( Gated Recurrent Unit - Unidade Recorrente com Portas))
- Aplicações práticas em **NLP** (Natural Language Processing - Processamento de Linguagem Natural) e previsão de séries temporais

## 🧠 Contexto Inicial: Dados Sequenciais

Antes de falarmos sobre RNNs, vamos entender por que os dados sequenciais são especiais.

### Exemplos de dados sequenciais:
- **Texto**: "O céu é azul" → ordem importa!
- **Séries temporais**: preços de ações ao longo dos dias
- **Áudio**: ondas sonoras em sequência
- **Vídeo**: frames em ordem cronológica

Em contraste com redes neurais tradicionais (como `MLP`), onde cada entrada é independente, nas `RNNs` **a ordem dos dados importa** e a rede precisa “lembrar” do que viu antes.

## 🔄 O Que São Redes Neurais Recorrentes (RNN)?

As **RNNs (Recurrent Neural Networks)** são redes neurais projetadas para trabalhar com **sequências**, mantendo uma espécie de **memória curta** sobre os dados anteriores.

### 🔁 Funcionamento Básico

A ideia central é que, a cada passo da sequência, a RNN recebe dois tipos de entrada:
1. **Dados atuais**: por exemplo, uma palavra ou valor no tempo t.
2. **Estado oculto anterior (hidden state)**: a memória da rede do passo anterior.

Esquema simplificado:

```
Entrada atual + Estado anterior → Processamento → Saída + Novo estado
```

## 📚 Exemplo Prático: Prever a próxima letra em uma frase

Vamos supor que queremos treinar uma RNN para prever a próxima letra em uma sequência.

Frase de exemplo: `"ola"`

Passos:
1. Entrada: `'o'` → Saída: `'l'`
2. Entrada: `'l'` → Saída: `'a'`
3. Entrada: `'a'` → Saída: `' '` (espaço)

Cada passo usa a informação do passo anterior para influenciar a previsão.

## ⚠️ Problemas Enfrentados: Gradiente Desvanecente

Embora RNNs sejam boas para capturar padrões sequenciais, elas têm um grande problema:

### 🌫️ O Problema do Gradiente Desvanecente

Durante o treinamento com **backpropagation through time**, os gradientes podem ficar muito pequenos conforme retrocedem no tempo, dificultando a atualização dos pesos associados aos primeiros elementos da sequência.

O Gradiente Desvanecente (ou Vanishing Gradient ) é um problema que ocorre durante o treinamento de redes neurais recorrentes (RNNs) e outras redes profundas, especialmente ao usar algoritmos de aprendizado como o **backpropagation through time**. 

Ele acontece quando os gradientes — que indicam a direção e a intensidade com que os pesos da rede devem ser ajustados para minimizar o erro — tornam-se extremamente pequenos à medida que são propagados do final para o início da rede, especialmente em sequências longas. 

Como resultado, os pesos das camadas iniciais praticamente não se atualizam, fazendo com que a rede pareça "estagnar" e falhe em aprender padrões importantes presentes nos dados mais antigos da sequência. 

Isso limita muito a capacidade da RNN de capturar dependências de longo prazo, como entender uma informação mencionada no começo de uma frase e usá-la para prever algo no final. Para resolver esse problema, surgiram arquiteturas mais avançadas, como LSTM e GRU , que possuem mecanismos internos para controlar melhor o fluxo de informações ao longo do tempo.

#### Consequência:
- A rede não consegue aprender relações de longo prazo.
- Exemplo: Se eu digo “Eu moro no Brasil há muitos anos...”, e depois pergunto “De qual país você é?”, a resposta depende do início da frase, mas a RNN pode esquecer isso se a sequência for longa.

## 🧠 Solução: Arquiteturas Avançadas – LSTM e GRU

Para resolver o problema do gradiente desvanecente e melhorar a capacidade de lembrança, surgiram variações mais complexas da RNN:

### 1️⃣ Long Short-Term Memory (LSTM)

A LSTM tem uma estrutura mais complexa, com portas (gates) que controlam o fluxo de informações:

- **Forget Gate**: Decide o que esquecer do estado anterior
- **Input Gate**: Decide o que adicionar à memória
- **Output Gate**: Decide o que mostrar como saída

Essas portas permitem que a LSTM aprenda quais informações manter ou descartar, permitindo lembranças de longo prazo.

**Long Short-Term Memory**, ou `LSTM`, é um tipo especial de camada em redes neurais recorrentes (RNNs) projetada para resolver o problema do gradiente desvanecente e permitir que a rede aprenda dependências de longo prazo em dados sequenciais. 

Diferentemente das **RNNs** tradicionais, que possuem dificuldade em lembrar informações ao longo de muitos passos de tempo, as **LSTMs** utilizam uma estrutura interna com portas controladas — a **forget gate**, **input gate** e **output gate** — que decidem o que manter, o que descartar e o que mostrar como saída. 

Isso permite que a LSTM mantenha uma memória de longo prazo de forma seletiva, facilitando o aprendizado de padrões complexos em sequências como textos, falas ou séries temporais, tornando-as **extremamente úteis em tarefas de Processamento de Linguagem Natural (NLP)**, **previsão de valores** futuros em séries temporais e outras aplicações onde a ordem e o contexto dos dados são fundamentais.

### 2️⃣ Gated Recurrent Unit (GRU)

É uma versão simplificada da LSTM. Tem menos parâmetros e também usa portas para controlar o fluxo de memória, combinando o **forget** e **input** gates em uma única porta de atualização.

Gated Recurrent Unit, ou GRU , é uma variação aprimorada das redes neurais recorrentes (RNNs) projetada para lidar com o problema do gradiente desvanecente e melhorar a capacidade de aprendizado em sequências longas, assim como a LSTM. 

Porém, a GRU possui uma arquitetura mais simples e eficiente: ela combina o estado da célula e o estado oculto em uma única estrutura e utiliza apenas duas portas — a `update gate` (porta de atualização) e a `reset gate` (porta de redefinição) — para controlar o fluxo de informações ao longo do tempo. 

A `update gate` decide quanto da informação anterior deve ser mantida, enquanto a `reset gate` determina quanta dessa informação antiga será ignorada ao calcular o novo estado. 

Isso torna a GRU **mais rápida para treinar e executar**, sendo uma excelente escolha quando se deseja um bom **equilíbrio entre capacidade de memória e eficiência computacional**, especialmente em tarefas como **modelagem de linguagem**, **tradução** e **previsão** de séries temporais.

## 💬 Aplicações Práticas

### 1. **Processamento de Linguagem Natural (NLP)**

#### Exemplos:
- **Tradução automática**: RNNs codificam uma frase em uma língua e decodificam em outra.
- **Geração de texto**: modelos como o Char-RNN geram texto caractere por caractere.
- **Análise de sentimentos**: classificar uma frase como positiva ou negativa.

#### Exemplo Simples: Análise de Sentimento

```python
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense

model = Sequential()
model.add(Embedding(input_dim=10000, output_dim=64))  # 10k palavras
model.add(SimpleRNN(units=64))
model.add(Dense(units=1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Suponha X_train e y_train já pré-processados
# model.fit(X_train, y_train, epochs=5, batch_size=32)
```

> Este modelo usa uma RNN simples para classificar frases como positivas ou negativas.

### 2. **Previsão de Séries Temporais**

Exemplo: prever preço de ações ou temperatura futura com base em dados históricos.

#### Exemplo Simples: Previsão de Temperatura

```python
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense
import numpy as np

# Dados simulados: sequência de temperaturas
temperaturas = np.array([20, 21, 22, 21, 23, 24, 25, 24, 25, 26])
X = temperaturas[:-1].reshape(1, -1, 1)  # [batch_size, timesteps, features]
y = temperaturas[1:]

model = Sequential()
model.add(SimpleRNN(units=10, input_shape=(None, 1)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=100, verbose=0)

# Fazer previsão
pred = model.predict(X)
print("Temperatura prevista:", pred.flatten())
```

> Aqui, usamos uma RNN para prever a próxima temperatura com base na sequência anterior.

## 🧩 Comparação entre Modelos

| Modelo | Memória | Complexidade | Uso Comum |
|--------|---------|--------------|-----------|
| RNN    | Curta   | Baixa        | Tarefas simples |
| LSTM   | Longa   | Alta         | NLP, séries longas |
| GRU    | Média/Longa | Média      | Menos custo computacional |

---

## 📚 Resumo Final

✅ As RNNs são redes neurais voltadas para **dados sequenciais**  
✅ Possuem **memória de curto prazo** através do estado oculto  
❌ Sofrem com o problema do **gradiente desvanecente**  
✅ LSTM e GRU resolvem esse problema com estruturas mais complexas  
🛠️ Aplicáveis a NLP, previsão de séries, geração de texto, etc.

## 📚 Sugestões de Estudo Adicional

- Implementar uma LSTM para classificação de sentimentos usando datasets famosos (ex: IMDB)
- Usar bibliotecas como `TensorFlow`, `Keras`, `PyTorch` para criar redes recorrentes
- Explorar arquiteturas seq2seq (encoder-decoder) para tradução de textos
- Estudar modelos como Transformer (sucessores das RNNs)

