# Redes Neurais Recorrentes (RNN)

## ğŸ¯ Objetivo da Aula

Nesta aula, vamos introduzir o conceito de **Redes Neurais Recorrentes (RNN)**, um tipo especial de rede neural projetada para **lidar com dados sequenciais**, como **textos e sÃ©ries temporais**. Ao final da aula, vocÃª entenderÃ¡:

- Como as RNNs funcionam
- Por que elas sÃ£o Ãºteis para sequÃªncias
- Os principais desafios do uso das RNN tradicionais
- SoluÃ§Ãµes avanÃ§adas como **LSTM** (Long Short-Term Memory - MemÃ³ria de Curto Prazo Longa) e **GRU** ( Gated Recurrent Unit - Unidade Recorrente com Portas))
- AplicaÃ§Ãµes prÃ¡ticas em **NLP** (Natural Language Processing - Processamento de Linguagem Natural) e previsÃ£o de sÃ©ries temporais

## ğŸ§  Contexto Inicial: Dados Sequenciais

Antes de falarmos sobre RNNs, vamos entender por que os dados sequenciais sÃ£o especiais.

### Exemplos de dados sequenciais:
- **Texto**: "O cÃ©u Ã© azul" â†’ ordem importa!
- **SÃ©ries temporais**: preÃ§os de aÃ§Ãµes ao longo dos dias
- **Ãudio**: ondas sonoras em sequÃªncia
- **VÃ­deo**: frames em ordem cronolÃ³gica

Em contraste com redes neurais tradicionais (como `MLP`), onde cada entrada Ã© independente, nas `RNNs` **a ordem dos dados importa** e a rede precisa â€œlembrarâ€ do que viu antes.

## ğŸ”„ O Que SÃ£o Redes Neurais Recorrentes (RNN)?

As **RNNs (Recurrent Neural Networks)** sÃ£o redes neurais projetadas para trabalhar com **sequÃªncias**, mantendo uma espÃ©cie de **memÃ³ria curta** sobre os dados anteriores.

### ğŸ” Funcionamento BÃ¡sico

A ideia central Ã© que, a cada passo da sequÃªncia, a RNN recebe dois tipos de entrada:
1. **Dados atuais**: por exemplo, uma palavra ou valor no tempo t.
2. **Estado oculto anterior (hidden state)**: a memÃ³ria da rede do passo anterior.

Esquema simplificado:

```
Entrada atual + Estado anterior â†’ Processamento â†’ SaÃ­da + Novo estado
```

## ğŸ“š Exemplo PrÃ¡tico: Prever a prÃ³xima letra em uma frase

Vamos supor que queremos treinar uma RNN para prever a prÃ³xima letra em uma sequÃªncia.

Frase de exemplo: `"ola"`

Passos:
1. Entrada: `'o'` â†’ SaÃ­da: `'l'`
2. Entrada: `'l'` â†’ SaÃ­da: `'a'`
3. Entrada: `'a'` â†’ SaÃ­da: `' '` (espaÃ§o)

Cada passo usa a informaÃ§Ã£o do passo anterior para influenciar a previsÃ£o.

## âš ï¸ Problemas Enfrentados: Gradiente Desvanecente

Embora RNNs sejam boas para capturar padrÃµes sequenciais, elas tÃªm um grande problema:

### ğŸŒ«ï¸ O Problema do Gradiente Desvanecente

Durante o treinamento com **backpropagation through time**, os gradientes podem ficar muito pequenos conforme retrocedem no tempo, dificultando a atualizaÃ§Ã£o dos pesos associados aos primeiros elementos da sequÃªncia.

O Gradiente Desvanecente (ou Vanishing Gradient ) Ã© um problema que ocorre durante o treinamento de redes neurais recorrentes (RNNs) e outras redes profundas, especialmente ao usar algoritmos de aprendizado como o **backpropagation through time**. 

Ele acontece quando os gradientes â€” que indicam a direÃ§Ã£o e a intensidade com que os pesos da rede devem ser ajustados para minimizar o erro â€” tornam-se extremamente pequenos Ã  medida que sÃ£o propagados do final para o inÃ­cio da rede, especialmente em sequÃªncias longas. 

Como resultado, os pesos das camadas iniciais praticamente nÃ£o se atualizam, fazendo com que a rede pareÃ§a "estagnar" e falhe em aprender padrÃµes importantes presentes nos dados mais antigos da sequÃªncia. 

Isso limita muito a capacidade da RNN de capturar dependÃªncias de longo prazo, como entender uma informaÃ§Ã£o mencionada no comeÃ§o de uma frase e usÃ¡-la para prever algo no final. Para resolver esse problema, surgiram arquiteturas mais avanÃ§adas, como LSTM e GRU , que possuem mecanismos internos para controlar melhor o fluxo de informaÃ§Ãµes ao longo do tempo.

#### ConsequÃªncia:
- A rede nÃ£o consegue aprender relaÃ§Ãµes de longo prazo.
- Exemplo: Se eu digo â€œEu moro no Brasil hÃ¡ muitos anos...â€, e depois pergunto â€œDe qual paÃ­s vocÃª Ã©?â€, a resposta depende do inÃ­cio da frase, mas a RNN pode esquecer isso se a sequÃªncia for longa.

## ğŸ§  SoluÃ§Ã£o: Arquiteturas AvanÃ§adas â€“ LSTM e GRU

Para resolver o problema do gradiente desvanecente e melhorar a capacidade de lembranÃ§a, surgiram variaÃ§Ãµes mais complexas da RNN:

### 1ï¸âƒ£ Long Short-Term Memory (LSTM)

A LSTM tem uma estrutura mais complexa, com portas (gates) que controlam o fluxo de informaÃ§Ãµes:

- **Forget Gate**: Decide o que esquecer do estado anterior
- **Input Gate**: Decide o que adicionar Ã  memÃ³ria
- **Output Gate**: Decide o que mostrar como saÃ­da

Essas portas permitem que a LSTM aprenda quais informaÃ§Ãµes manter ou descartar, permitindo lembranÃ§as de longo prazo.

**Long Short-Term Memory**, ou `LSTM`, Ã© um tipo especial de camada em redes neurais recorrentes (RNNs) projetada para resolver o problema do gradiente desvanecente e permitir que a rede aprenda dependÃªncias de longo prazo em dados sequenciais. 

Diferentemente das **RNNs** tradicionais, que possuem dificuldade em lembrar informaÃ§Ãµes ao longo de muitos passos de tempo, as **LSTMs** utilizam uma estrutura interna com portas controladas â€” a **forget gate**, **input gate** e **output gate** â€” que decidem o que manter, o que descartar e o que mostrar como saÃ­da. 

Isso permite que a LSTM mantenha uma memÃ³ria de longo prazo de forma seletiva, facilitando o aprendizado de padrÃµes complexos em sequÃªncias como textos, falas ou sÃ©ries temporais, tornando-as **extremamente Ãºteis em tarefas de Processamento de Linguagem Natural (NLP)**, **previsÃ£o de valores** futuros em sÃ©ries temporais e outras aplicaÃ§Ãµes onde a ordem e o contexto dos dados sÃ£o fundamentais.

### 2ï¸âƒ£ Gated Recurrent Unit (GRU)

Ã‰ uma versÃ£o simplificada da LSTM. Tem menos parÃ¢metros e tambÃ©m usa portas para controlar o fluxo de memÃ³ria, combinando o **forget** e **input** gates em uma Ãºnica porta de atualizaÃ§Ã£o.

Gated Recurrent Unit, ou GRU , Ã© uma variaÃ§Ã£o aprimorada das redes neurais recorrentes (RNNs) projetada para lidar com o problema do gradiente desvanecente e melhorar a capacidade de aprendizado em sequÃªncias longas, assim como a LSTM. 

PorÃ©m, a GRU possui uma arquitetura mais simples e eficiente: ela combina o estado da cÃ©lula e o estado oculto em uma Ãºnica estrutura e utiliza apenas duas portas â€” a `update gate` (porta de atualizaÃ§Ã£o) e a `reset gate` (porta de redefiniÃ§Ã£o) â€” para controlar o fluxo de informaÃ§Ãµes ao longo do tempo. 

A `update gate` decide quanto da informaÃ§Ã£o anterior deve ser mantida, enquanto a `reset gate` determina quanta dessa informaÃ§Ã£o antiga serÃ¡ ignorada ao calcular o novo estado. 

Isso torna a GRU **mais rÃ¡pida para treinar e executar**, sendo uma excelente escolha quando se deseja um bom **equilÃ­brio entre capacidade de memÃ³ria e eficiÃªncia computacional**, especialmente em tarefas como **modelagem de linguagem**, **traduÃ§Ã£o** e **previsÃ£o** de sÃ©ries temporais.

## ğŸ’¬ AplicaÃ§Ãµes PrÃ¡ticas

### 1. **Processamento de Linguagem Natural (NLP)**

#### Exemplos:
- **TraduÃ§Ã£o automÃ¡tica**: RNNs codificam uma frase em uma lÃ­ngua e decodificam em outra.
- **GeraÃ§Ã£o de texto**: modelos como o Char-RNN geram texto caractere por caractere.
- **AnÃ¡lise de sentimentos**: classificar uma frase como positiva ou negativa.

#### Exemplo Simples: AnÃ¡lise de Sentimento

```python
from keras.models import Sequential
from keras.layers import Embedding, SimpleRNN, Dense

model = Sequential()
model.add(Embedding(input_dim=10000, output_dim=64))  # 10k palavras
model.add(SimpleRNN(units=64))
model.add(Dense(units=1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Suponha X_train e y_train jÃ¡ prÃ©-processados
# model.fit(X_train, y_train, epochs=5, batch_size=32)
```

> Este modelo usa uma RNN simples para classificar frases como positivas ou negativas.

### 2. **PrevisÃ£o de SÃ©ries Temporais**

Exemplo: prever preÃ§o de aÃ§Ãµes ou temperatura futura com base em dados histÃ³ricos.

#### Exemplo Simples: PrevisÃ£o de Temperatura

```python
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense
import numpy as np

# Dados simulados: sequÃªncia de temperaturas
temperaturas = np.array([20, 21, 22, 21, 23, 24, 25, 24, 25, 26])
X = temperaturas[:-1].reshape(1, -1, 1)  # [batch_size, timesteps, features]
y = temperaturas[1:]

model = Sequential()
model.add(SimpleRNN(units=10, input_shape=(None, 1)))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse')
model.fit(X, y, epochs=100, verbose=0)

# Fazer previsÃ£o
pred = model.predict(X)
print("Temperatura prevista:", pred.flatten())
```

> Aqui, usamos uma RNN para prever a prÃ³xima temperatura com base na sequÃªncia anterior.

## ğŸ§© ComparaÃ§Ã£o entre Modelos

| Modelo | MemÃ³ria | Complexidade | Uso Comum |
|--------|---------|--------------|-----------|
| RNN    | Curta   | Baixa        | Tarefas simples |
| LSTM   | Longa   | Alta         | NLP, sÃ©ries longas |
| GRU    | MÃ©dia/Longa | MÃ©dia      | Menos custo computacional |

---

## ğŸ“š Resumo Final

âœ… As RNNs sÃ£o redes neurais voltadas para **dados sequenciais**  
âœ… Possuem **memÃ³ria de curto prazo** atravÃ©s do estado oculto  
âŒ Sofrem com o problema do **gradiente desvanecente**  
âœ… LSTM e GRU resolvem esse problema com estruturas mais complexas  
ğŸ› ï¸ AplicÃ¡veis a NLP, previsÃ£o de sÃ©ries, geraÃ§Ã£o de texto, etc.

## ğŸ“š SugestÃµes de Estudo Adicional

- Implementar uma LSTM para classificaÃ§Ã£o de sentimentos usando datasets famosos (ex: IMDB)
- Usar bibliotecas como `TensorFlow`, `Keras`, `PyTorch` para criar redes recorrentes
- Explorar arquiteturas seq2seq (encoder-decoder) para traduÃ§Ã£o de textos
- Estudar modelos como Transformer (sucessores das RNNs)

