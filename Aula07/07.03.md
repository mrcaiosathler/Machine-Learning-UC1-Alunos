# IntroduÃ§Ã£o Ã  Arquitetura Transformer

## ğŸ¯ Objetivos da Aula:

- **Entender** o que Ã© a arquitetura Transformer
- Conhecer o conceito do **mecanismo de atenÃ§Ã£o**
- Compreender a diferenÃ§a entre **encoder** e **decoder**
- Ver como essa arquitetura revolucionou o NLP
- Conhecer aplicaÃ§Ãµes reais como **BERT**, **GPT** e **traduÃ§Ã£o automÃ¡tica**

---

## 1. ğŸ“š O Que Ã© a Arquitetura Transformer?

A **Transformer** Ã© um modelo de `deep learning` introduzido em 2017 no artigo intitulado **"Attention Is All You Need"** pelos pesquisadores do `Google Brain` e do `OpenAI`.

Antes dos `Transformers`, modelos como **RNNs** (Redes Neurais Recorrentes) e **LSTMs** eram usados para tarefas de **NLP**, mas tinham limitaÃ§Ãµes:

- Processavam texto **sequencialmente** (palavra por palavra)
- Tinham **dificuldade em capturar relaÃ§Ãµes distantes** no texto (problema de longa dependÃªncia)

### âœ¨ O Que Mudou com os Transformers?

Os Transformers **nÃ£o processam dados sequencialmente**, mas sim de forma paralela, usando um mecanismo chamado **atenÃ§Ã£o** (`attention`).

> **Resumo:** Transformers mudaram o jogo ao permitir processamento paralelo e captura de relaÃ§Ãµes complexas entre palavras.

## 2. ğŸ” Mecanismo de AtenÃ§Ã£o

O coraÃ§Ã£o dos Transformers Ã© o **mecanismo de atenÃ§Ã£o**. Ele permite ao modelo "prestar atenÃ§Ã£o" nas partes mais relevantes de uma frase ao interpretÃ¡-la.

### Exemplo PrÃ¡tico:

Vamos analisar esta frase:

> **"O gato dormiu na cama porque estava cansado."**

Se eu perguntar:  
**"Quem estava cansado?"**

Um modelo bem treinado saberÃ¡ que "cansado" se refere a **"gato"**, nÃ£o a "cama". Como ele faz isso? Usando atenÃ§Ã£o!

### Funcionamento BÃ¡sico da AtenÃ§Ã£o:

- Cada palavra tem trÃªs vetores associados:
  - **Query (Q)** â€“ o que estÃ¡ buscando
  - **Key (K)** â€“ o que representa
  - **Value (V)** â€“ o que traz de informaÃ§Ã£o

Esses vetores sÃ£o combinados para calcular quais palavras estÃ£o mais relacionadas entre si.

#### FÃ³rmula Simples (para entender):

```python
AtenÃ§Ã£o(Q, K, V) = softmax(Q Â· K^T / âˆšd_k) Â· V
```

Onde `d_k` Ã© a dimensÃ£o das chaves, para evitar valores muito grandes.

> ğŸ’¡ **Dica:** Imagine que vocÃª estÃ¡ numa festa e precisa conversar com vÃ¡rias pessoas. VocÃª escolhe com quem prestar atenÃ§Ã£o com base em como elas se conectam com vocÃª â€” Ã© isso que o mecanismo de atenÃ§Ã£o faz!

---

## 3. ğŸ—ï¸ Arquitetura Encoder-Decoder

A arquitetura `Transformer` original tem duas partes principais:

- **Encoder**
- **Decoder**

### ğŸ” Encoder

**Recebe o texto** de entrada e **extrai caracterÃ­sticas** importantes. NÃ£o gera saÃ­da diretamente, apenas entende o contexto.

- Um Transformer pode ter **vÃ¡rios blocos encadeados** de encoder (geralmente 6).
- Cada bloco contÃ©m:
  - Camada de **atenÃ§Ã£o multi-head**
  - Camada de **feed-forward**
  - NormalizaÃ§Ã£o e conexÃµes residuais

### ğŸ”„ Decoder

Usa as informaÃ§Ãµes do encoder para **gerar uma saÃ­da** (como traduzir uma frase ou continuar um texto).

- TambÃ©m tem vÃ¡rios blocos (geralmente 6)
- AlÃ©m da atenÃ§Ã£o normal, usa **atenÃ§Ã£o mascarada** para evitar ver o futuro da sequÃªncia

### Exemplo Visual:

```
[Input] â†’ [Embedding] â†’ [Encoder] â†’ [Decoder] â†’ [Output]
```

**Exemplo real**: TraduÃ§Ã£o de InglÃªs para FrancÃªs.

---

## 4. ğŸŒŸ Modelos Baseados em Transformers: BERT e GPT

Do `Transformer` surgiram dois dos modelos mais famosos do mundo do NLP:

### ğŸ“˜ BERT (Bidirectional Encoder Representations from Transformers)

- Usa apenas o **encoder**
- Ã‰ **bidirecional**: lÃª o texto completo de uma vez, entendendo o contexto antes e depois de cada palavra
- Excelente para tarefas como **classificaÃ§Ã£o de sentimentos**, **question answering** e **anÃ¡lise de texto**

#### Exemplo PrÃ¡tico:

```python
from transformers import BertTokenizer, TFBertForMaskedLM
import tensorflow as tf

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')

text = f"Paris is the capital of [MASK]."
tokenized_input = tokenizer(text, return_tensors='tf')
mask_token_index = tf.where(tokenized_input["input_ids"][0] == tokenizer.mask_token_id)

logits = model(tokenized_input).logits
mask_token_logits = logits[0, mask_token_index, :]
top_tokens = tf.math.top_k(mask_token_logits, k=5)

for token in top_tokens.indices[0].numpy():
    print(f"Palavra sugerida: {tokenizer.decode([token])}")
```

> SaÃ­da esperada: Palavras como "France", "country", etc.

---

### ğŸ§  GPT (Generative Pre-trained Transformer)

- Usa apenas o **decoder**
- **Gera texto** de forma **autoregressiva** (uma palavra apÃ³s outra)
- Ideal para geraÃ§Ã£o de textos criativos, chatbots, resumos

#### Exemplo PrÃ¡tico:

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

prompt = "The future of artificial intelligence is"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)

print(tokenizer.decode(outputs[0]))
```

> SaÃ­da possÃ­vel: "...promising and full of possibilities. It will change how we live..."

## 5. ğŸŒ AplicaÃ§Ãµes Reais dos Transformers

Os `Transformers` estÃ£o por toda parte hoje. Veja algumas Ã¡reas onde eles se destacam:

| Ãrea | AplicaÃ§Ã£o | Modelo Exemplo |
|------|-----------|----------------|
| TraduÃ§Ã£o | Traduzir idiomas automaticamente | Transformer Base |
| GeraÃ§Ã£o de Texto | Escrever histÃ³rias, emails, artigos | GPT, GPT-2, GPT-3 |
| Question Answering | Responder perguntas em textos | BERT |
| ClassificaÃ§Ã£o de Sentimentos | Analisar emoÃ§Ã£o em textos | DistilBERT |
| SumarizaÃ§Ã£o | Criar resumos automÃ¡ticos | BART |
| Reconhecimento de Entidades Nomeadas | Identificar pessoas, locais, organizaÃ§Ãµes | SpaCy + Transformers |

## 6. ğŸ§ª Atividade PrÃ¡tica: Traduzindo com Transformers

Vamos criar um exemplo simples de traduÃ§Ã£o de inglÃªs para portuguÃªs usando `Hugging Face Transformers`.

### CÃ³digo:

```python
from transformers import pipeline

translator = pipeline("translation_en_to_pt", model="Helsinki-NLP/opus-mt-en-pt")

sentence = "Artificial Intelligence is transforming the world."
translated = translator(sentence, max_length=40)

print(translated[0]['translation_text'])
```

> SaÃ­da esperada: "A inteligÃªncia artificial estÃ¡ transformando o mundo."

## 7. ğŸ“š Recapitulando

- **Transformers** revolucionaram o NLP graÃ§as ao mecanismo de atenÃ§Ã£o
- Eles permitem **processamento paralelo** e compreensÃ£o contextual avanÃ§ada
- SÃ£o divididos em **encoder** (entende o texto) e **decoder** (gera texto)
- Modelos como **BERT** (usam `encoder`) e **GPT** (usam `decoder`) sÃ£o super populares
- Podem ser usados para **traduÃ§Ã£o, geraÃ§Ã£o de texto, sumÃ¡rio, classificaÃ§Ã£o**, etc.

## 8. ğŸ“š Material Adicional

- Livro: *Deep Learning for NLP* â€“ com foco em Transformers
- DocumentaÃ§Ã£o: [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- Curso Gratuito: [Natural Language Processing with Transformers (Hugging Face)](https://course.fast.ai/)

## 9. ğŸ§© Desafios para FixaÃ§Ã£o

1. Use o `BERT` para preencher uma lacuna em uma frase em portuguÃªs.
2. Use o `GPT` para gerar uma histÃ³ria curta a partir de um inÃ­cio dado.
3. Traduza um pequeno texto do portuguÃªs para o inglÃªs usando `Transformers`.
4. Compare a saÃ­da de um modelo `LSTM` e um `Transformer` para uma mesma tarefa.
