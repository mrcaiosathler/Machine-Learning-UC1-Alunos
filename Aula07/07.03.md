# Introdução à Arquitetura Transformer

## 🎯 Objetivos da Aula:

- **Entender** o que é a arquitetura Transformer
- Conhecer o conceito do **mecanismo de atenção**
- Compreender a diferença entre **encoder** e **decoder**
- Ver como essa arquitetura revolucionou o NLP
- Conhecer aplicações reais como **BERT**, **GPT** e **tradução automática**

---

## 1. 📚 O Que é a Arquitetura Transformer?

A **Transformer** é um modelo de `deep learning` introduzido em 2017 no artigo intitulado **"Attention Is All You Need"** pelos pesquisadores do `Google Brain` e do `OpenAI`.

Antes dos `Transformers`, modelos como **RNNs** (Redes Neurais Recorrentes) e **LSTMs** eram usados para tarefas de **NLP**, mas tinham limitações:

- Processavam texto **sequencialmente** (palavra por palavra)
- Tinham **dificuldade em capturar relações distantes** no texto (problema de longa dependência)

### ✨ O Que Mudou com os Transformers?

Os Transformers **não processam dados sequencialmente**, mas sim de forma paralela, usando um mecanismo chamado **atenção** (`attention`).

> **Resumo:** Transformers mudaram o jogo ao permitir processamento paralelo e captura de relações complexas entre palavras.

## 2. 🔍 Mecanismo de Atenção

O coração dos Transformers é o **mecanismo de atenção**. Ele permite ao modelo "prestar atenção" nas partes mais relevantes de uma frase ao interpretá-la.

### Exemplo Prático:

Vamos analisar esta frase:

> **"O gato dormiu na cama porque estava cansado."**

Se eu perguntar:  
**"Quem estava cansado?"**

Um modelo bem treinado saberá que "cansado" se refere a **"gato"**, não a "cama". Como ele faz isso? Usando atenção!

### Funcionamento Básico da Atenção:

- Cada palavra tem três vetores associados:
  - **Query (Q)** – o que está buscando
  - **Key (K)** – o que representa
  - **Value (V)** – o que traz de informação

Esses vetores são combinados para calcular quais palavras estão mais relacionadas entre si.

#### Fórmula Simples (para entender):

```python
Atenção(Q, K, V) = softmax(Q · K^T / √d_k) · V
```

Onde `d_k` é a dimensão das chaves, para evitar valores muito grandes.

> 💡 **Dica:** Imagine que você está numa festa e precisa conversar com várias pessoas. Você escolhe com quem prestar atenção com base em como elas se conectam com você — é isso que o mecanismo de atenção faz!

---

## 3. 🏗️ Arquitetura Encoder-Decoder

A arquitetura `Transformer` original tem duas partes principais:

- **Encoder**
- **Decoder**

### 🔁 Encoder

**Recebe o texto** de entrada e **extrai características** importantes. Não gera saída diretamente, apenas entende o contexto.

- Um Transformer pode ter **vários blocos encadeados** de encoder (geralmente 6).
- Cada bloco contém:
  - Camada de **atenção multi-head**
  - Camada de **feed-forward**
  - Normalização e conexões residuais

### 🔄 Decoder

Usa as informações do encoder para **gerar uma saída** (como traduzir uma frase ou continuar um texto).

- Também tem vários blocos (geralmente 6)
- Além da atenção normal, usa **atenção mascarada** para evitar ver o futuro da sequência

### Exemplo Visual:

```
[Input] → [Embedding] → [Encoder] → [Decoder] → [Output]
```

**Exemplo real**: Tradução de Inglês para Francês.

---

## 4. 🌟 Modelos Baseados em Transformers: BERT e GPT

Do `Transformer` surgiram dois dos modelos mais famosos do mundo do NLP:

### 📘 BERT (Bidirectional Encoder Representations from Transformers)

- Usa apenas o **encoder**
- É **bidirecional**: lê o texto completo de uma vez, entendendo o contexto antes e depois de cada palavra
- Excelente para tarefas como **classificação de sentimentos**, **question answering** e **análise de texto**

#### Exemplo Prático:

```python
from transformers import BertTokenizer, TFBertForMaskedLM
import tensorflow as tf

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')

text = f"Paris is the capital of [MASK]."
tokenized_input = tokenizer(text, return_tensors='tf')
mask_token_index = tf.where(tokenized_input["input_ids"][0] == tokenizer.mask_token_id)

logits = model(tokenized_input).logits
mask_token_logits = logits[0, mask_token_index, :]
top_tokens = tf.math.top_k(mask_token_logits, k=5)

for token in top_tokens.indices[0].numpy():
    print(f"Palavra sugerida: {tokenizer.decode([token])}")
```

> Saída esperada: Palavras como "France", "country", etc.

---

### 🧠 GPT (Generative Pre-trained Transformer)

- Usa apenas o **decoder**
- **Gera texto** de forma **autoregressiva** (uma palavra após outra)
- Ideal para geração de textos criativos, chatbots, resumos

#### Exemplo Prático:

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

prompt = "The future of artificial intelligence is"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=50)

print(tokenizer.decode(outputs[0]))
```

> Saída possível: "...promising and full of possibilities. It will change how we live..."

## 5. 🌐 Aplicações Reais dos Transformers

Os `Transformers` estão por toda parte hoje. Veja algumas áreas onde eles se destacam:

| Área | Aplicação | Modelo Exemplo |
|------|-----------|----------------|
| Tradução | Traduzir idiomas automaticamente | Transformer Base |
| Geração de Texto | Escrever histórias, emails, artigos | GPT, GPT-2, GPT-3 |
| Question Answering | Responder perguntas em textos | BERT |
| Classificação de Sentimentos | Analisar emoção em textos | DistilBERT |
| Sumarização | Criar resumos automáticos | BART |
| Reconhecimento de Entidades Nomeadas | Identificar pessoas, locais, organizações | SpaCy + Transformers |

## 6. 🧪 Atividade Prática: Traduzindo com Transformers

Vamos criar um exemplo simples de tradução de inglês para português usando `Hugging Face Transformers`.

### Código:

```python
from transformers import pipeline

translator = pipeline("translation_en_to_pt", model="Helsinki-NLP/opus-mt-en-pt")

sentence = "Artificial Intelligence is transforming the world."
translated = translator(sentence, max_length=40)

print(translated[0]['translation_text'])
```

> Saída esperada: "A inteligência artificial está transformando o mundo."

## 7. 📚 Recapitulando

- **Transformers** revolucionaram o NLP graças ao mecanismo de atenção
- Eles permitem **processamento paralelo** e compreensão contextual avançada
- São divididos em **encoder** (entende o texto) e **decoder** (gera texto)
- Modelos como **BERT** (usam `encoder`) e **GPT** (usam `decoder`) são super populares
- Podem ser usados para **tradução, geração de texto, sumário, classificação**, etc.

## 8. 📚 Material Adicional

- Livro: *Deep Learning for NLP* – com foco em Transformers
- Documentação: [Hugging Face Transformers](https://huggingface.co/docs/transformers/)
- Curso Gratuito: [Natural Language Processing with Transformers (Hugging Face)](https://course.fast.ai/)

## 9. 🧩 Desafios para Fixação

1. Use o `BERT` para preencher uma lacuna em uma frase em português.
2. Use o `GPT` para gerar uma história curta a partir de um início dado.
3. Traduza um pequeno texto do português para o inglês usando `Transformers`.
4. Compare a saída de um modelo `LSTM` e um `Transformer` para uma mesma tarefa.
