# Redes Neurais com PyTorch

## **Objetivo da Aula**
Nesta aula, vamos abordaremos o conceito de **redes neurais artificiais** (`RNAs`), explicando como elas funcionam e por que são tão poderosas em problemas complexos de **machine learning**. Além disso, mostraremos como implementar uma rede neural simples no **PyTorch**, uma biblioteca popular para **deep learning**.


## **1. O Que São Redes Neurais Artificiais?**

As redes neurais são modelos computacionais inspirados no cérebro humano. Elas são compostas por **neurônios artificiais** conectados entre si, organizados em **camadas**:

- **Camada de entrada**: recebe os dados.
- **Camadas ocultas**: processam as informações.
- **Camada de saída**: produz o resultado final.

### **Exemplo Simples: Prever Se Um Aluno Vai Passar ou Não**

Suponha que temos um conjunto de dados com notas de provas, presença e horas de estudo. Queremos prever se um aluno vai passar ou não.

| Nota P1 | Presença (%) | Horas Estudo | Resultado |
|--------|--------------|--------------|-----------|
| 7.0    | 95           | 12           | Aprovado  |
| 4.5    | 60           | 5            | Reprovado |

Uma rede neural pode aprender padrões desses dados para fazer previsões futuras.

## **2. Como Funciona Uma Rede Neural?**

Cada neurônio realiza uma operação simples:

1. **Pondera as entradas**: multiplica cada valor de entrada por um peso.
2. **Soma os valores ponderados**.
3. **Aplica uma função de ativação** (como `ReLU` ou `Sigmoid`) para decidir se o neurônio "dispara" (ativa).

**Explicação**:

- **ReLU (Rectified Linear Unit)**
    - é uma das funções de ativação mais usadas em redes neurais. 
    - Ela retorna como saída o valor de entrada diretamente se ele for positivo ou zero; caso contrário, retorna zero. Em outras palavras, "ativa" o neurônio apenas quando a entrada é positiva. 
    - Sua simplicidade e eficiência no treinamento de redes profundas a tornam muito popular, especialmente em camadas ocultas, pois evita o problema do **gradiente desvanecido** que ocorre com outras funções como a `sigmoid`.
    
- **Sigmoid**
    - é outra função de ativação comum, que transforma qualquer valor de entrada em um número entre 0 e 1. 
    - Por isso, ela é frequentemente usada na camada de saída de redes neurais para problemas de classificação binária, onde o resultado representa uma probabilidade.
    - Apesar de útil, a sigmoid pode causar problemas no treinamento de redes profundas devido ao chamado problema do gradiente desvanecido, já que seus gradientes tendem a ficar muito pequenos para valores extremos de entrada.

### **Exemplo Matemático de Um Neurônio**

Dado:
- Entradas: x₁ = 0.8, x₂ = 0.3
- Pesos: w₁ = 0.4, w₂ = 0.6
- Viés (bias): b = 0.1

Calculamos:

$$
\text{soma} = (x_1 \cdot w_1) + (x_2 \cdot w_2) + b = (0.8 * 0.4) + (0.3 * 0.6) + 0.1 = 0.32 + 0.18 + 0.1 = 0.6
$$

Se usarmos a função **ReLU**, o resultado é:

$$
\text{saída} = \max(0, 0.6) = 0.6
$$

---

## **3. Implementando Uma Rede Neural Simples no PyTorch**

Vamos criar uma rede neural muito simples usando o **PyTorch** para **classificar dados** gerados artificialmente.

### **Passo 1: Importando Bibliotecas**

```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
```

### **Passo 2: Gerando Dados de Exemplo**

```python
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2)
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # Transformar em coluna
```

### **Passo 3: Dividindo Treino e Teste**

```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

### **Passo 4: Definindo a Rede Neural**

```python
class SimpleNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.camada1 = nn.Linear(10, 16)  # Camada de entrada (10 features) -> 16 neurônios
        self.relu = nn.ReLU()
        self.camada2 = nn.Linear(16, 1)   # Camada oculta -> saída (1 neurônio)

    def forward(self, x):
        x = self.relu(self.camada1(x))
        x = torch.sigmoid(self.camada2(x))  # Função sigmoide na saída para probabilidade
        return x
```

### **Passo 5: Criando Modelo, Função de Perda e Otimizador**

```python
model = SimpleNN()
criterion = nn.BCELoss()  # Binary Cross Entropy Loss
optimizer = optim.Adam(model.parameters(), lr=0.01)
```

### **Passo 6: Treinando o Modelo**

```python
epochs = 100
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()

    output = model(X_train)
    loss = criterion(output, y_train)
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'Época {epoch+1}, Perda: {loss.item():.4f}')
```

### **Passo 7: Avaliando o Modelo**

```python
model.eval()
with torch.no_grad():
    pred = model(X_test)
    pred_class = (pred > 0.5).float()
    acc = (pred_class == y_test).sum().item() / len(y_test)
    print(f'Acurácia: {acc:.2%}')
```

---

## **4. Conexão com Aulas Anteriores**

Em aulas anteriores, vimos técnicas de **pré-processamento de dados**, como:

- **Normalização** de características
- **Codificação** de variáveis categóricas
- Separação dos conjuntos de **treino e teste**

Essas etapas são **essenciais** para o bom desempenho das redes neurais. Por exemplo:

- **Normalizar os dados** ajuda a evitar explosão de gradientes durante o treinamento.
- **Codificação correta de variáveis** evita interpretações errôneas pelos neurônios.
- **Dividir os dados** garante que o modelo generalize bem para novos exemplos.

O **PyTorch** facilita essas tarefas com estruturas flexíveis e funções prontas para carregamento e transformação de dados.

## **5. Aplicações Práticas de Redes Neurais**

Redes neurais são extremamente versáteis e podem ser aplicadas em diversos domínios:

### **Processamento de Linguagem Natural (PLN)**

- **Classificação** de sentimentos
- **Tradução** automática
- **Geração** de textos

### **Análise de Séries Temporais**

- **Previsão** de vendas
- **Detecção** de anomalias em sensores
- **Previsão** de ações na bolsa

### **Classificação e Regressão Complexa**

- **Diagnóstico** médico com base em exames
- **Detecção** de fraudes
- **Estimativa** de preços de imóveis

### **Clusterização e Autoencoders**

- **Agrupamento** de clientes
- **Redução** de dimensionalidade
- **Detecção** de `outliers`


## **6. Quando Utilizar Redes Neurais?**

### **Situações Adequadas**

- Grandes volumes de dados (imagens, textos, vídeos)
- Relações **não lineares** **complexas** entre variáveis
- Tarefas com **alta complexidade** sem regras claras

### **Limitações**

- Necessitam de **muitos recursos computacionais**
- Podem ser **difíceis de interpretar**
- Requerem **ajuste fino de hiperparâmetros** (número de camadas, neurônios, taxa de aprendizado etc.)

### **Requisitos Básicos**

- Hardware adequado (GPU é recomendado)
- Tempo de treinamento maior do que métodos mais simples
- Domínio do pré-processamento e ajuste de arquitetura


## **7. Considerações Finais**

Combinando **pré-processamento eficiente**, **arquiteturas bem projetadas** e **treinamento cuidadoso**, podemos resolver problemas que antes eram impossíveis com métodos tradicionais de machine learning.

Na próxima aula, vamos explorar **redes neurais convolucionais (CNNs)** e seu uso em imagens!

---

## **Atividade Extra para Fixação**

Implemente uma rede neural para **prever o preço** de imóveis com o dataset do **Boston Housing** no PyTorch. **Normalize** os dados e use uma função de perda apropriada para regressão (`MSELoss`).

:bulb: **Explicação**:

- **MSELoss (Mean Squared Error Loss)**
    - A função MSELoss, ou Erro Médio Quadrático, é uma métrica comumente usada em problemas de regressão para medir o desempenho de um modelo. 
    - Ela calcula a média dos quadrados das diferenças entre os valores reais (alvo) e os valores previstos pelo modelo. Quanto menor o valor da MSELoss, melhor o modelo está ajustando-se aos dados. 
    - No contexto de redes neurais, ela é amplamente utilizada como função de perda para tarefas que preveem valores numéricos contínuos, como prever preços, temperaturas ou qualquer variável real.