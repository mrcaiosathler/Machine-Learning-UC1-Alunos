# Fundamentos Essenciais do PyTorch  

> Nosso objetivo √©
> 
> - Entender os fundamentos do **PyTorch**, uma biblioteca poderosa para **deep learning**.
> - Comparar o **PyTorch** com o **Scikit-learn**, destacando suas diferen√ßas e vantagens.
> - Desenvolver um modelo simples de rede neural usando **tensores**, **autograd**, **fun√ß√µes de perda**, **otimizadores** e **treinamento em GPU**.
> - Aprender conceitos importantes como **forward pass**, **backward pass**, **batch size**, **epochs**, etc.
> 
> Ao final, voc√™ ser√° capaz de construir e treinar seu primeiro modelo de rede neural com PyTorch.
> 
> ---

## üîç Introdu√ß√£o ao PyTorch

O **PyTorch** √© uma biblioteca desenvolvida pelo Facebook (Meta) para computa√ß√£o num√©rica e modelagem baseada em **redes neurais**. √â amplamente usada em pesquisas acad√™micas e **aplica√ß√µes reais de intelig√™ncia artificial**.

Os modelos neurais , tamb√©m chamados de redes neurais artificiais (RNAs) , s√£o sistemas computacionais inspirados no funcionamento do c√©rebro humano. Eles s√£o usados para reconhecer padr√µes, tomar decis√µes e aprender a partir de dados.

### üì¶ Estrutura B√°sica de uma Rede Neural Artificial

Uma rede neural √© composta por:

#### 1. **Camada de Entrada (Input Layer)**  
Recebe os dados brutos ‚Äî como pixels de uma imagem, palavras de um texto ou valores num√©ricos.

#### 2. **Camadas Ocultas (Hidden Layers)**  
S√£o camadas intermedi√°rias onde acontece o processamento real da rede. Cada neur√¥nio nessa camada:
- Recebe entradas ponderadas (multiplicadas pelos pesos).
- Soma essas entradas.
- Aplica uma **fun√ß√£o de ativa√ß√£o** para decidir se "dispara" ou n√£o (como um neur√¥nio biol√≥gico).

#### 3. **Camada de Sa√≠da (Output Layer)**  
Produz o resultado final do modelo ‚Äî por exemplo, a classe prevista em um problema de classifica√ß√£o ou um valor num√©rico em uma regress√£o.

---

### üßÆ Exemplo Pr√°tico: Rede Neural para Classificar Animais

Suponha que queremos criar um modelo que diga se uma foto mostra um **gato** ou um **cachorro**.

- **Entrada**: pixels da imagem (por exemplo, 784 n√∫meros para uma imagem 28x28).
- **Camadas ocultas**: v√°rias camadas de neur√¥nios que aprendem caracter√≠sticas como ‚Äútem orelhas pontudas‚Äù, ‚Äútem focinho longo‚Äù, etc.
- **Sa√≠da**: um n√∫mero que indica a probabilidade de ser um gato ou cachorro.

### Por Que Usar PyTorch?

- **Flexibilidade**: Voc√™ pode definir modelos neurais customizados com liberdade.
- **Gr√°fico Din√¢mico**: Diferente de frameworks como TensorFlow (que usa gr√°ficos est√°ticos), o PyTorch executa opera√ß√µes em tempo real ‚Äî ou seja, voc√™ v√™ o que acontece √† medida que programa.
- **Suporte a GPU**: Ideal para acelerar c√°lculos complexos, especialmente em grandes volumes de dados.
- **Integra√ß√£o com outras ferramentas**: Compat√≠vel com **datasets**, visualiza√ß√µes, e at√© integra√ß√£o com Python puro.

## üß† Conceito 1: Tensores ‚Äì A Base do PyTorch

Os **tensores** s√£o a unidade fundamental do PyTorch. S√£o **similares aos arrays** do NumPy, mas com suporte a GPU.

### Exemplos B√°sicos

```python
import torch

"""
Tensor 1D (vetor)
- Criamos um tensor 1D (um vetor). Ele tem tr√™s elementos: `[1, 2, 3]`. 
- O comando `print()` mostra o conte√∫do na tela.
"""
tensor_1d = torch.tensor([1, 2, 3])
print("Tensor 1D:", tensor_1d)

"""
# Tensor 2D (matriz)
- Este √© um tensor 2D (uma matriz). Tem duas linhas e duas colunas:
"""
tensor_2d = torch.tensor([[1, 2], [3, 4]])
print("\nTensor 2D:\n", tensor_2d)

"""
# Tensor aleat√≥rio 3x3
- Gera um tensor 3x3 com valores aleat√≥rios entre 0 e 1. √ötil para inicializar pesos de redes neurais.
"""
tensor_rand = torch.rand(3, 3)
print("\nTensor aleat√≥rio 3x3:\n", tensor_rand)

"""
# Tensor zeros 2x2
-  Cria um tensor 2x2 preenchido com zeros. Muitas vezes usado como placeholder ou inicializa√ß√£o.
"""
tensor_zeros = torch.zeros(2, 2)
print("\nTensor de zeros 2x2:\n", tensor_zeros)

"""
# Tensor com n√∫meros aleat√≥rios normalizados
- Gera valores aleat√≥rios com m√©dia 0 e desvio padr√£o 1 (distribui√ß√£o normal). Tamb√©m √∫til para inicializar pesos.
"""
tensor_normal = torch.randn(2, 3)
print("\nTensor normalmente distribu√≠do:\n", tensor_normal)
```

### Opera√ß√µes B√°sicas com Tensores

Voc√™ pode somar, multiplicar, concatenar tensores:

```python
a = torch.tensor([1, 2])
b = torch.tensor([3, 4])

# Soma
soma = a + b
print("Soma:", soma)

# Multiplica√ß√£o elemento a elemento
mult = a * b
print("Multiplica√ß√£o:", mult)

# Produto interno (dot product)
dot = torch.dot(a, b)
print("Produto interno:", dot)
```


## üìà Conceito 2: Autograd ‚Äì Diferencia√ß√£o Autom√°tica

No deep learning, precisamos calcular **gradientes das fun√ß√µes** para atualizar os **pesos dos modelos**. O m√≥dulo `autograd` do PyTorch faz isso automaticamente.


### Como Funciona?

Quando voc√™ define um tensor com `requires_grad=True`, o PyTorch come√ßa a rastrear todas as opera√ß√µes realizadas nele.

#### Exemplo Simples

```python
x = torch.tensor(2.0, requires_grad=True)
y = x ** 2  # y = x^2

# Calculando derivada dy/dx
y.backward()
print("Derivada de y = x¬≤ em x=2:", x.grad)  # Deveria ser 4.0
```

‚úÖ **Explica√ß√£o:** 

- Criamos um tensor com valor `2.0` e ativamos o rastreamento de gradiente (`requires_grad=True`). Isso permite calcular derivadas automaticamente.

- Definimos uma fun√ß√£o matem√°tica: `y = x¬≤`.

- Chamamos `.backward()` para calcular o gradiente de `y` em rela√ß√£o a `x`.

-  O gradiente de `y = x¬≤` √© `dy/dx = 2x`. Quando `x = 2`, o resultado √© `4.0`.


> üí° **Entendendo:** Nesse caso, `y = x¬≤`, ent√£o `dy/dx = 2x`. Quando `x = 2`, o resultado √© `4`.

Esse mecanismo √© usado durante o treinamento de redes neurais para ajustar os pesos com base nos erros calculados.


## ü§ñ Conceito 3: Modelos Neurais ‚Äì Criando sua Primeira Rede

Vamos criar um modelo muito simples: uma rede neural com uma camada linear.

### Passo a Passo

1. Importe o m√≥dulo `torch.nn`.
2. Crie uma classe herdando de `nn.Module`.
3. Defina as camadas no m√©todo `__init__`.
4. Implemente o fluxo direto (`forward`) da rede.

#### Exemplo: Rede Neural Linear

```python
import torch.nn as nn

class SimpleLinearModel(nn.Module):
    def __init__(self):
        super(SimpleLinearModel, self).__init__()
        # Camada linear: entrada (10 features) ‚Üí sa√≠da (1 valor)
        self.linear = nn.Linear(10, 1)

    def forward(self, x):
        return self.linear(x)
```

‚úÖ **Explica√ß√£o:** 
- Importamos o m√≥dulo `nn`, que cont√©m classes e fun√ß√µes para criar redes neurais.
- Definimos uma classe chamada `SimpleLinearModel` que herda de `nn.Module`. No m√©todo `__init__`, criamos uma camada linear (`nn.Linear`) que recebe 10 caracter√≠sticas e retorna 1 valor (√∫til para regress√£o).
- O m√©todo `forward` define como os dados fluem atrav√©s da rede. Neste caso, eles passam diretamente pela camada linear.

Este modelo recebe vetores com 10 caracter√≠sticas e retorna um √∫nico n√∫mero (√∫til para regress√£o).

## üéØ Conceito 4: Fun√ß√£o de Perda (Loss Function)

A fun√ß√£o de perda mede o qu√£o longe as previs√µes do modelo est√£o do valor real. Um exemplo comum √© o **MSE (Mean Squared Error)**.

### Exemplo:

```python
criterion = nn.MSELoss()  # Erro quadr√°tico m√©dio

# Suponha:
predicao = torch.tensor([[2.0]])
real = torch.tensor([[3.0]])

loss = criterion(predicao, real)
print("Erro (Loss):", loss.item())
```

‚úÖ **Explica√ß√£o:** 

- Estamos usando o erro quadr√°tico m√©dio como m√©trica de erro. Ele compara a sa√≠da do modelo com o valor real.

- Aqui, simulamos uma previs√£o do modelo (`2.0`) e um valor real (`3.0`). O erro √© calculado como `(2 - 3)^2 = 1`. O m√©todo `.item()` extrai o valor escalar do tensor.


> üí° **Interpreta√ß√£o:** Quanto menor o valor do erro, melhor o modelo est√° se ajustando aos dados.

---

## ‚öôÔ∏è Conceito 5: Otimizador ‚Äì Atualizando os Pesos

Para minimizar a fun√ß√£o de perda, usamos otimizadores como **SGD (Stochastic Gradient Descent)** ou **Adam**.

### Exemplo:

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
```

- `model.parameters()` ‚Üí todos os pesos e bias do modelo.
- `lr=0.01` ‚Üí taxa de aprendizado (learning rate).

‚úÖ **Explica√ß√£o:** 
- Estamos usando o otimizador SGD (Gradiente Descendente Estoc√°stico). Ele atualiza os par√¢metros do modelo (`model.parameters()`) com taxa de aprendizado `lr=0.01`.


## üèãÔ∏è‚Äç‚ôÇÔ∏è Conceito 6: Treinamento do Modelo

Agora vamos juntar tudo: criamos um modelo, carregamos os dados e treinamos por algumas √©pocas.

### Exemplo Completo

```python
# Dados simulados
X_train = torch.randn(100, 10)  # 100 amostras, cada uma com 10 features
y_train = torch.randn(100, 1)   # 100 valores alvo

# Instanciando o modelo
model = SimpleLinearModel()

# Fun√ß√£o de perda e otimizador
criterion = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# Treinamento
for epoch in range(100):
    # Forward pass
    outputs = model(X_train)
    loss = criterion(outputs, y_train)

    # Backward pass e otimiza√ß√£o
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 10 == 0:
        print(f'√âpoca {epoch+1}, Erro: {loss.item():.4f}')
```

 ‚úÖ **Explica√ß√£o:** 
 
 - Geramos dados aleat√≥rios para simular um dataset. Cada amostra tem 10 caracter√≠sticas (`X_train`) e um valor alvo (`y_train`).
 
 -  Criamos uma inst√¢ncia do nosso modelo definido anteriormente.
 
 - Definimos a fun√ß√£o de perda e o otimizador.
 
 - Para cada √©poca:
    1. **Forward pass**: o modelo faz previs√µes com base nos dados.
    1. **C√°lculo do erro**: comparamos a previs√£o com o valor real.

 - `zero_grad()`: zeramos os gradientes acumulados para evitar interfer√™ncias entre √©pocas.
 
 - `loss.backward()`: calculamos os gradientes do erro em rela√ß√£o aos pesos.
 
 - `optimizer.step()`: atualizamos os pesos com base nos gradientes.

-  A cada 10 √©pocas, imprimimos o n√∫mero da √©poca e o valor do erro, formatado com 4 casas decimais.


### O Que Aconteceu Aqui?

1. **Forward pass**: o modelo fez previs√µes com base nos dados.
2. **C√°lculo do erro**: comparou-se a previs√£o com o valor real.
3. **Backward pass**: calcularam-se os gradientes do erro em rela√ß√£o aos pesos.
4. **Atualiza√ß√£o dos pesos**: os pesos foram ajustados com base nesses gradientes.
5. **Repeti√ß√£o**: esse processo foi repetido v√°rias vezes (√©pocas) para melhorar o modelo.

## üñ•Ô∏è Conceito 7: Execu√ß√£o em GPU ‚Äì Acelerando o Processamento

Se voc√™ tem uma placa de v√≠deo compat√≠vel (NVIDIA CUDA), pode mover todo o processamento para a GPU.

### Verificando Dispositivos Dispon√≠veis

- Verifica se h√° uma GPU dispon√≠vel. Se houver, usaremos `'cuda'`; caso contr√°rio, `'cpu'`.

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Dispositivo usado:", device)
```

### Movendo Modelo e Dados para GPU

- Movemos o modelo e os dados para o dispositivo selecionado (GPU ou CPU). Todos devem estar no mesmo local para evitar erros.

```python
model.to(device)
X_train = X_train.to(device)
y_train = y_train.to(device)
```

> üí° **Importante:** Todos os tensores devem estar no mesmo dispositivo (CPU ou GPU) para evitar erros.

---

## üìä Compara√ß√£o entre PyTorch e Scikit-learn

| Caracter√≠stica             | **Scikit-learn**                                  | **PyTorch**                                               |
|---------------------------|---------------------------------------------------|----------------------------------------------------------|
| Tipo de aprendizado       | Machine Learning tradicional                    | Deep Learning e computa√ß√£o tensorial                     |
| Estrutura                 | APIs prontas e fixas                              | Arquiteturas flex√≠veis e customiz√°veis                   |
| Gr√°ficos de computa√ß√£o    | N√£o h√°                                            | Gr√°fico din√¢mico (`define-by-run`)                       |
| Autodifera√ß√£o             | N√£o suporta                                       | Sim (via `autograd`)                                     |
| Execu√ß√£o em GPU           | N√£o                                               | Sim                                                       |
| Complexidade              | Baixa (ideal para iniciantes)                     | M√©dia-alta (requer conhecimento de DL)                   |
| Frameworks associados     | Pandas, Matplotlib, Seaborn                       | TorchVision, TorchText, Fastai                           |
| Desempenho em grandes dados | Adequado para datasets m√©dios                  | Ideal para datasets grandes e modelos complexos          |

> ‚úÖ **Resumo Pr√°tico:** Use **Scikit-learn** para tarefas simples de ML (como classifica√ß√£o e regress√£o com m√©todos cl√°ssicos). Use **PyTorch** quando quiser trabalhar com redes neurais profundas, especialmente com imagens, texto ou dados sequenciais.

---

## üöÄ Pr√≥ximos Passos Ap√≥s Este Tutorial

Ap√≥s dominar os conceitos b√°sicos do PyTorch, voc√™ poder√°:

1. **Avan√ßar em arquiteturas mais complexas**, como CNNs (Redes Convolucionais) e RNNs (Redes Recorrentes).
2. **Usar datasets reais** com bibliotecas como `torchvision` e `torch.utils.data`.
3. **Visualizar resultados** com matplotlib/seaborn.
4. **Salvar e recarregar modelos** para uso posterior.
5. **Fazer deploy em produ√ß√£o** com ferramentas como TorchScript.

---

## üìù Resumo Final ‚Äì Termos-Chave

| Termo            | Significado                                                                 |
|------------------|------------------------------------------------------------------------------|
| Tensor           | Estrutura b√°sica do PyTorch, similar a arrays NumPy                        |
| Autograd         | Mecanismo para c√°lculo autom√°tico de gradientes                            |
| Forward pass     | Etapa onde o modelo faz previs√µes                                          |
| Backward pass    | Etapa onde os gradientes s√£o calculados                                    |
| Loss             | Medida de erro entre predi√ß√£o e valor real                                 |
| Optimizer        | Ferramenta que atualiza os pesos do modelo                                 |
| Epoch            | Uma passagem completa pelos dados de treino                                |
| Batch Size       | N√∫mero de amostras usadas em cada itera√ß√£o                                 |
| GPU              | Plataforma para acelera√ß√£o de c√°lculos com suporte no PyTorch              |

---

## üìå Exerc√≠cios Sugeridos

1. Modifique o modelo para ter duas camadas lineares.
2. Adicione uma fun√ß√£o de ativa√ß√£o ReLU entre as camadas.
3. Tente executar o c√≥digo em GPU e compare o tempo de execu√ß√£o.
4. Substitua o otimizador SGD por Adam e veja se o desempenho melhora.
5. Altere a taxa de aprendizado e observe o impacto no erro.

---

## üìö Recomenda√ß√µes Finais

- **Documenta√ß√£o Oficial do PyTorch**: https://pytorch.org/docs/stable/index.html
- **Tutoriais Oficiais**: https://pytorch.org/tutorials/
- **Livros recomendados**:  
  - *Deep Learning with PyTorch* ‚Äì Eli Stevens et al.  
  - *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* ‚Äì Aur√©lien G√©ron (para compreender contextos ML/DL)
