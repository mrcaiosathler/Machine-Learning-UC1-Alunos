# Bibliotecas Pandas e Scikit-learn

## **1. Apresenta√ß√£o das Bibliotecas**

### **Pandas**

O **Pandas** √© uma biblioteca do Python usada principalmente para **manipula√ß√£o e an√°lise de dados**. Ele permite trabalhar com tabelas (chamadas de *DataFrames*), carregar dados de diferentes fontes (como CSV, Excel, SQL, etc.), fazer **filtros**, **agrupamentos**, **tratamento** de valores ausentes, entre outros.

### **Scikit-learn (sklearn)**

O **Scikit-learn** √© uma biblioteca de **machine learning** que oferece ferramentas prontas para:

- **Pr√©-processamento** de dados

- **Modelagem estat√≠stica** 
  
  - Processo de usar t√©cnicas e m√©todos estat√≠sticos para **descrever, analisar e fazer previs√µes** com base em dados. 
  
  - O objetivo √© **encontrar rela√ß√µes** entre vari√°veis e **entender padr√µes** nos dados, muitas vezes criando um modelo matem√°tico que represente essas rela√ß√µes. 
  
  - Esse tipo de modelagem pode ser usada tanto para **explicar fen√¥menos** (como identificar fatores que influenciam uma determinada resposta) quanto para **prever resultados** futuros (como estimar vendas ou classificar categorias). 
  
  - A modelagem estat√≠stica est√° presente em diversas √°reas, como economia, sa√∫de, engenharia e ci√™ncias sociais, e serve como **base** para muitos algoritmos de **machine learning**.

- **Treinamento de modelos preditivos (regress√£o, classifica√ß√£o, clustering)**
  
  - √â o processo de **ensinar um algoritmo** computacional a **reconhecer** padr√µes nos dados para fazer **previs√µes** sobre informa√ß√µes futuras ou desconhecidas. 
  
  - Esse treinamento pode ser dividido em **tr√™s grandes categorias**: 
  
    - **Regress√£o** : usada para **prever valores** num√©ricos cont√≠nuos, como prever o pre√ßo de uma casa; 
    
    - **Classifica√ß√£o** : que busca **identificar** a qual **categoria** ou classe um novo dado pertence, como determinar se um e-mail √© spam ou n√£o; 
    
    - **Clustering** : que **agrupa dados semelhantes** entre si sem a necessidade de r√≥tulos pr√©vios, sendo √∫til para **segmenta√ß√£o** de clientes ou **detec√ß√£o de padr√µes** ocultos. 
    
  - Durante o treinamento, o modelo ajusta seus **par√¢metros** com base em um conjunto de **dados conhecido** (dados de treino), buscando minimizar os erros nas previs√µes e generalizar bem para novos dados.

- **Avalia√ß√£o de modelos**:
    
    - Processo de medir e **analisar o desempenho de um modelo** de machine learning ap√≥s seu treinamento, para garantir que ele seja eficaz e **confi√°vel ao fazer previs√µes** em dados novos e n√£o vistos. 
    
    - Esse passo √© essencial para **verificar** se o modelo **aprendeu** corretamente os padr√µes dos dados de treino e se consegue generalizar bem, ou seja, evitar problemas como **overfitting** (quando o modelo memoriza os dados de treino e falha nos novos dados) ou **underfitting** (quando o modelo n√£o captura nenhum padr√£o relevante). 
    
    - Para isso, s√£o utilizadas **m√©tricas** espec√≠ficas, como **acur√°cia**, **precis√£o**, **recall** e **F1-score** para classifica√ß√£o, e erro m√©dio absoluto (MAE), erro quadr√°tico m√©dio (MSE) ou R¬≤ para regress√£o. Al√©m disso, t√©cnicas como valida√ß√£o cruzada ajudam a obter uma estimativa mais robusta do desempenho do modelo.


- **Valida√ß√£o cruzada**: 
    - T√©cnica estat√≠stica usada para **avaliar** e comparar o **desempenho** de modelos de machine learning, garantindo que eles sejam **capazes de generalizar** bem para dados novos e n√£o vistos. 
    
    - Em vez de dividir os dados apenas em conjuntos de treino e teste uma √∫nica vez, a valida√ß√£o cruzada **divide os dados em v√°rias partes** (ou "folds"), treinando e testando o modelo m√∫ltiplas vezes com diferentes combina√ß√µes dessas partes. 
    
    - Isso permite obter **uma m√©dia mais precisa** do desempenho do modelo, reduzindo o impacto de varia√ß√µes aleat√≥rias nos dados. 
    
    - A forma mais comum √© a **k-fold cross-validation**, onde os dados s√£o divididos em **k subconjuntos** iguais, e o modelo √© treinado em k-1 deles e testado no restante, repetindo-se esse processo k vezes. 
    
    - Assim, a valida√ß√£o cruzada ajuda a evitar superajuste (overfitting ) e oferece uma vis√£o mais confi√°vel da efic√°cia do modelo.

- **Sele√ß√£o de caracter√≠sticas** :

    - Tamb√©m conhecida como **feature selection**, √© o processo de **escolher o conjunto** mais relevante e significativo de vari√°veis (ou caracter√≠sticas) de um dataset para a constru√ß√£o de um modelo de machine learning. 
    
    - O objetivo √© **identificar** as **colunas** que t√™m maior **poder preditivo**, descartando aquelas que n√£o contribuem ou at√© prejudicam o desempenho do modelo. 
    
    - Isso **ajuda** a **reduzir** a **complexidade** do modelo, **evitar overfitting** (ajuste excessivo aos dados de treino), melhorar a velocidade de treinamento e aumentar a interpretabilidade do modelo. 
    
    - As **t√©cnicas** de sele√ß√£o de caracter√≠sticas podem ser divididas em:
        
        - **m√©todos filtros** - baseados em estat√≠sticas dos dados;
        
        - **wrapper** - usam o pr√≥prio modelo para avaliar subconjuntos de features.
        
        - **embedded** - incorporam a sele√ß√£o durante o treinamento do modelo, como em √°rvores de decis√£o ou regress√£o com regulariza√ß√£o. 
        
    - Essa etapa √© **fundamental no pr√©-processamento** de dados para construir modelos mais eficientes e confi√°veis.

Essas duas bibliotecas s√£o frequentemente utilizadas juntas em projetos de an√°lise de dados e modelagem preditiva.

## **2. Objetivo da Aula**

Nesta aula, vamos revisar o processo de:

1. Carregar e explorar um dataset usando o **Pandas**
2. Identificar e tratar valores ausentes
3. Detectar e lidar com **outliers**
4. Trabalhar com vari√°veis **categ√≥ricas**
5. Preparar os dados para uso em algoritmos de **machine learning** com o **Scikit-learn**

Vamos usar um exemplo pr√°tico com o seguinte dataset:

| id  | idade | renda | cidade         | categoria | nota | feedback |
| --- | ----- | ----- | -------------- | --------- | ---- | -------- |
| 1   | 28    | 5000  | S√£o Paulo      | A         | 7.5  | Bom      |
| 2   | NaN   | 3500  | Rio de Janeiro | B         | 6.2  | Regular  |
| 3   | 45    | NaN   | Belo Horizonte | A         | 9.0  | Bom      |
| 4   | 32    | 4200  | S√£o Paulo      | B         | NaN  | Ruim     |

---

## **3. Passo a Passo com Exemplos Pr√°ticos**

Vamos come√ßar importando as bibliotecas necess√°rias:

:snake: Codigo:

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
```

:books: **Bibliotecas**

- **pandas** 
    - O Pandas √© uma biblioteca essencial para **manipula√ß√£o e an√°lise** de dados em Python. Ele permite trabalhar com estruturas de dados como o DataFrame , que √© semelhante a uma tabela do Excel ou um banco de dados relacional.
    
- **numpy**
    - O NumPy √© uma biblioteca fundamental para **computa√ß√£o cient√≠fica** em Python. Ela fornece suporte para **arrays multidimensionais** e **fun√ß√µes matem√°ticas** avan√ßadas.

- **StandardScaler**
    -  **Padronizar** (ou normalizar) as vari√°veis num√©ricas, ou seja, transformar os valores para que tenham **m√©dia zero e desvio padr√£o igual a 1** .
    - Muitos algoritmos de machine learning (como KNN, Regress√£o Log√≠stica e SVM) funcionam melhor quando os dados est√£o na mesma escala.
    
```python
scaler = StandardScaler()
dados_padronizados = scaler.fit_transform(dados_numericos)
```
    
- **OneHotEncoder**
    - Converter **vari√°veis categ√≥ricas** (como "S√£o Paulo", "Rio de Janeiro") **em n√∫meros bin√°rios** (0s e 1s ) para que possam ser usadas em modelos de machine learning.
    - Modelos de ML n√£o conseguem entender texto diretamente; eles precisam de dados num√©ricos.
    
```python
encoder = OneHotEncoder()
categorias_codificadas = encoder.fit_transform(df[['cidade']])
```
    
-  **SimpleImputer**

    - Tratar **valores ausentes** (tamb√©m chamados de NaN ou null) nos dados.
    - Permite substituir valores faltantes pela **m√©dia**, **mediana**, **moda** ou um **valor constante**.
    - Algoritmos de machine learning geralmente **n√£o aceitam dados faltantes**, ent√£o √© necess√°rio preench√™-los com algum valor razo√°vel.
    
  ```python
  imputer = SimpleImputer(strategy='mean')  # preenche com a m√©dia
  df[['idade']] = imputer.fit_transform(df[['idade']])
  ```
Essas bibliotecas juntas formam uma base s√≥lida para pr√©-processar dados antes de aplicar modelos de machine learning.

### **Passo 1: Carregar o Dataset**

Suponha que o dataset est√° salvo em um arquivo chamado `dados.csv`.

:snake: C√≥digo

```python
df = pd.read_csv('dados.csv')
print(df.head())
```
> üìå **Explica√ß√£o:** Estamos lendo o arquivo CSV e armazenando seus dados em um DataFrame chamado `df`. O m√©todo `.head()` mostra as primeiras linhas do dataset.


### **Passo 2: Explorar o Dataset**

#### Verificar tipos de dados e informa√ß√µes gerais:

```python
print(df.info())
```

Isso mostrar√°:

- Quantidade de linhas e colunas
- Tipo de cada coluna
- N√∫mero de valores n√£o nulos

#### Estat√≠sticas descritivas (para colunas num√©ricas):

```python
print(df.describe())
```

> üìå **Explica√ß√£o:** Isso ajuda a entender melhor a distribui√ß√£o dos dados num√©ricos, como m√©dia, desvio padr√£o, valores m√≠nimos e m√°ximos.

---

### **Passo 3: Identificar Valores Ausentes**

Para identificar campos faltantes:

```python
print(df.isnull().sum())
```

> üìå **Explica√ß√£o:** O m√©todo `.isnull()` retorna True onde h√° valores ausentes. Usando `.sum()`, contamos quantos valores faltantes existem por coluna.

Exemplo de sa√≠da:

```
id          0
idade       1
renda       1
cidade      0
categoria   0
nota        1
feedback    0
```

### **Passo 4: Preencher Valores Ausentes (Imputa√ß√£o)**

Vamos preencher os valores ausentes nas colunas **num√©ricas**: `idade` e `nota`.

Usaremos a **m√©dia** para preencher esses campos:

```python
imputer_media = SimpleImputer(strategy='mean')
df[['idade', 'nota']] = imputer_media.fit_transform(df[['idade', 'nota']])
```

Para a coluna `renda`, poder√≠amos usar a mediana se houver muitos outliers:

```python
imputer_mediana = SimpleImputer(strategy='median')
df[['renda']] = imputer_mediana.fit_transform(df[['renda']])
```

#### :bulb: Uso do **SimpleImputer**

 O **SimpleImputer** √© uma classe do Scikit-learn usada para **substituir** valores ausentes (**NaN**) em conjuntos de dados. Ele permite preencher os valores faltantes com diferentes estrat√©gias.

Voc√™ pode escolher a estrat√©gia de imputa√ß√£o usando o argumento `strategy` na cria√ß√£o do `SimpleImputer`. As op√ß√µes mais comuns s√£o:

| Estrat√©gia (`strategy`) | Descri√ß√£o |
|-------------------------|-----------|
| `'mean'`                | Substitui os valores ausentes pela **m√©dia** da coluna (apenas para dados num√©ricos). √ötil quando os dados t√™m distribui√ß√£o normal.|
| `'median'`              | Substitui os valores ausentes pela **mediana** da coluna (apenas para dados num√©ricos). Boa op√ß√£o quando h√° **outliers**, pois a mediana √© menos afetada por valores extremos. |
| `'most_frequent'`       | Substitui os valores ausentes pela **moda** (valor mais frequente) da coluna. Funciona para **dados num√©ricos e categ√≥ricos**. Ideal para **vari√°veis categ√≥ricas** ou num√©ricas com distribui√ß√£o assim√©trica. |
| `'constant'`            | Substitui os valores ausentes por um valor constante definido pelo usu√°rio, atrav√©s do par√¢metro `fill_value`. Funciona para qualquer tipo de dado. Voc√™ define qual valor usar. Pode ser √∫til para indicar que um valor est√° ausente (ex: `-999`, `"desconhecido"`). |

üîß **Exemplos de uso**

```python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
imputer = SimpleImputer(strategy='median')
imputer = SimpleImputer(strategy='most_frequent')
imputer = SimpleImputer(strategy='constant', fill_value=0)
imputer = SimpleImputer(strategy='constant', fill_value='Desconhecido')
```

üìå **Observa√ß√µes importantes:**

- O `SimpleImputer` **n√£o modifica diretamente** o DataFrame. Voc√™ precisa aplic√°-lo com `.fit_transform()` em seus dados.
- Para colunas categ√≥ricas, prefira `strategy='most_frequent'` ou `strategy='constant'`.
- A imputa√ß√£o deve ser feita **somente nos dados de treino**, para evitar **vazamento de dados** (*data leakage*) durante a modelagem preditiva.


:dart: M√©todos suportados

Os objetos da classe `SimpleImputer` do **Scikit-learn** possuem v√°rios m√©todos √∫teis para o processo de **imputa√ß√£o de valores ausentes**.

**üîß 1. `.fit(X)`**

- **Finalidade:**  
  Ajusta (treina) o imputador aos dados fornecidos.

- **O que ele faz?**  
  Calcula os valores a serem usados para preencher os dados faltantes, com base na estrat√©gia escolhida (`mean`, `median`, `most_frequent`, `constant`).

- **Exemplo:**
  ```python
  imputer = SimpleImputer(strategy='mean')
  imputer.fit(X_train)
  ```

> O m√©todo `.fit()` **n√£o modifica os dados**, apenas aprende os valores necess√°rios (ex: m√©dia de cada coluna).


üîÑ **2. `.transform(X)`**

- **Finalidade:**  
  Aplica a imputa√ß√£o nos dados, substituindo os valores ausentes pelos valores aprendidos no `.fit()`.

- **O que ele retorna?**  
  Um novo conjunto de dados (geralmente como um array NumPy), com os valores faltantes preenchidos.

- **Exemplo:**
  ```python
  X_treino_limpo = imputer.transform(X_train)
  ```

> Importante: Use `.transform()` somente **ap√≥s** ter usado `.fit()`.


üöÄ **3. `.fit_transform(X)`**

- **Finalidade:**  
  Combina√ß√£o de `.fit()` + `.transform()` em uma √∫nica etapa.

- **Quando usar?**  
  Durante o treinamento do modelo, especialmente com os dados de treino.

- **Exemplo:**
  ```python
  X_treino_limpo = imputer.fit_transform(X_train)
  ```

> √â mais eficiente e usado com frequ√™ncia em pipelines e pr√©-processamento inicial.

üìä **4. `.get_feature_names_out()`**

- **Finalidade:**  
  Retorna os nomes das colunas ap√≥s a transforma√ß√£o (√∫til quando usado em **pipelines** ou com outras transforma√ß√µes).

- **Exemplo:**
  ```python
  feature_names = imputer.get_feature_names_out()
  df_limpo = pd.DataFrame(X_treino_limpo, columns=feature_names)
  ```

> Este m√©todo √© √∫til para manter o nome das colunas ap√≥s a transforma√ß√£o


üìù **5. `.set_params()`**

- **Finalidade:**  
  Altera dinamicamente os par√¢metros do imputador (como a estrat√©gia ou valor constante).

- **Exemplo:**
  ```python
  imputer.set_params(strategy='median', fill_value=None)
  ```

> √ötil se quiser reutilizar o mesmo objeto imputador com diferentes configura√ß√µes.

üìã **6. `.statistics_`**

- **Finalidade:**  
  Atributo (n√£o um m√©todo) que armazena os valores calculados durante o `.fit()` para cada coluna.

- **O que cont√©m?**  
  Os valores usados para substituir os NaNs, como:
  - M√©dia de cada coluna (se `strategy='mean'`)
  - Mediana (se `strategy='median'`)
  - Moda (se `strategy='most_frequent'`)
  - Valor constante (se `strategy='constant'`)

- **Exemplo:**
  ```python
  print(imputer.statistics_)
  ```

> Isso √© √∫til para auditoria ou documenta√ß√£o: voc√™ pode ver exatamente quais valores foram usados na imputa√ß√£o.


‚úÖ Resumo dos M√©todos Mais Usados

| M√©todo / Atributo        | Descri√ß√£o |
|--------------------------|-----------|
| `.fit(X)`                | Aprende os valores para substitui√ß√£o (ex: m√©dia, mediana) |
| `.transform(X)`          | Aplica a substitui√ß√£o nos dados |
| `.fit_transform(X)`      | Faz `.fit()` e `.transform()` juntos |
| `.get_feature_names_out()` | Retorna os nomes das colunas ap√≥s transforma√ß√£o |
| `.set_params()`          | Muda par√¢metros do imputador |
| `.statistics_`           | Mostra os valores usados na substitui√ß√£o |

### **Passo 5: Identificar e Lidar com Outliers**

Vamos verificar poss√≠veis **outliers** na coluna `idade`.

#### Visualiza√ß√£o simples com boxplot:

```python
import matplotlib.pyplot as plt
plt.boxplot(df['idade'])
plt.title("Boxplot - Idade")
plt.show()
```

üìå **Explicando**:

Este c√≥digo √© usado para criar e exibir um gr√°fico de caixa (`boxplot`) da coluna **idade** de um DataFrame chamado df.

- A biblioteca **Matplotlib** , mais especificamente o m√≥dulo `pyplot`,  √© usado para criar gr√°ficos em 2D no Python. Usamos este m√≥dulo para visualizar dados graficamente, como histogramas, gr√°ficos de linhas, dispers√£o e boxplots .

- **Boxplot** √© um tipo de gr√°fico que mostra a distribui√ß√£o dos dados com base nos quartis . Ele ajuda a identificar:
    - A mediana (valor central)
    - Os quartis (25%, 50%, 75%)
    - Valores extremos (outliers )
    - A caixa representa os valores entre o 1¬∫ quartil (**Q1**) e o 3¬∫ quartil (**Q3**).
    - A linha dentro da caixa √© a **mediana** .
    - As "extremidades" das linhas ("**bigodes**") indicam os valores **m√≠nimos** e **m√°ximos** que n√£o s√£o outliers .
    - Pontos fora dessas extremidades s√£o considerados outliers .
    
    ![Boxplot - Exemplo](img/g2.png)

- Matplotlib constr√≥i o gr√°fico em mem√≥ria primeiro, e plt.show() √© o comando que realmente mostra o gr√°fico.

‚úÖ **Resumo** 

| Linha | C√≥digo                            | O que faz                                               |
|-------|-----------------------------------|---------------------------------------------------------|
| 1     | `import matplotlib.pyplot as plt` | Importa a biblioteca de gr√°ficos                        |
| 2     | `plt.boxplot(df['idade'])`        | Gera o gr√°fico de caixa com os dados da coluna 'idade'  |
| 3     | `plt.title("Boxplot - Idade")`    | Define o t√≠tulo do gr√°fico                              |
| 4     | `plt.show()`                      | Mostra o gr√°fico na tela                                |


#### Tratando os Outliers

Se encontrarmos outliers, podemos remover ou limitar os valores extremos:

```python
Q1 = df['idade'].quantile(0.25)
Q3 = df['idade'].quantile(0.75)
IQR = Q3 - Q1

limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

df = df[(df['idade'] >= limite_inferior) & (df['idade'] <= limite_superior)]
```

> üìå **Explica√ß√£o:** Acima, calculamos os quartis e removemos registros fora do intervalo considerado normal.

#### Reverificando com boxplot:

```python
import matplotlib.pyplot as plt
plt.boxplot(df['idade'])
plt.title("Boxplot - Idade")
plt.show()
```

### **Passo 6: Transformar Vari√°veis Categ√≥ricas**

As vari√°veis categ√≥ricas precisam ser convertidas em n√∫meros para os algoritmos de ML entenderem.

#### Op√ß√£o 1: One-Hot Encoding (para vari√°veis nominais)

```python
encoder = OneHotEncoder(sparse_output=False, drop='first')  
categorias_codificadas = encoder.fit_transform(df[['cidade', 'categoria', 'feedback']])

# Converter os resultados do encoding para um DataFrame
df_categorias = pd.DataFrame(categorias_codificadas, columns=encoder.get_feature_names_out(['cidade', 'categoria', 'feedback']))

# Resetar o √≠ndice do df original e concatenar com as novas colunas
df_final = pd.concat([df.reset_index(drop=True), df_categorias], axis=1)

# Remover as colunas categ√≥ricas originais (sem usar inplace)
df_final = df_final.drop(['cidade', 'categoria', 'feedback'], axis=1)
```

üìå **Explica√ß√£o:** 

O `OneHotEncoding` cria uma nova coluna bin√°ria para cada categoria. Por exemplo, "S√£o Paulo", "Rio de Janeiro" viram novas colunas com 0 ou 1.

O primeiro comando cria uma inst√¢ncia do **OneHotEncoder**, que ser√° usada para transformar vari√°veis categ√≥ricas em colunas bin√°rias (0 ou 1).

- **sparse_output=False**: retorna um **array NumPy** normal (n√£o esparsa), mais f√°cil de trabalhar.

- **drop='first'**: remove a primeira categoria de cada vari√°vel para evitar multicolinearidade (muito √∫til em modelos como Regress√£o Linear).

O `fit_transform` ajusta o encoder √†s colunas categ√≥ricas (fit) e depois aplica a transforma√ß√£o (transform), gerando um array com as categorias convertidas em n√∫meros. Ao final temos um array NumPy chamado **categorias_codificadas** com as novas colunas criadas pelo One-Hot Encoding.

O comando `pd.DataFrame` converte o array **categorias_codificadas** em um novo DataFrame chamado **df_categorias**.

J√° o m√©todo `get_feature_names_out()` gera nomes descritivos para as novas colunas, como: **cidade_Sao Paulo**, **categoria_B**, **feedback_Ruim**.

O `pd.concat(..)` concatenando o novo DataFrame com o original

- **reset_index(drop=True)** : Reinicia os √≠ndices do DataFrame original (df) para garantir que ele combine corretamente com o novo (df_categorias).

- **pd.concat(..., axis=1)** : Une os dois DataFrames lado a lado (colunas do original + colunas codificadas).

Finalmente `df_final.drop(...)`, remove as colunas categ√≥ricas originais (`cidade`, `categoria`, `feedback`) do DataFrame final.

:bulb:  **Por que n√£o usar `inplace=True`?**  
- Porque queremos seguir o estilo funcional, onde sempre atribu√≠mos o resultado √† vari√°vel. Isso ajuda a evitar efeitos colaterais e facilita o rastreamento de altera√ß√µes no c√≥digo.

üí° Visualizando o conte√∫do do DataFrame final:

```python
print(df_final.head())
```

Isso mostrar√° as primeiras linhas do `df_final`, com as novas colunas criadas pelo **One-Hot Encoder**.

### **Passo 7: Normalizar ou Padronizar Dados Num√©ricos**

Alguns algoritmos de ML funcionam melhor com dados padronizados (m√©dia zero e desvio padr√£o 1).

```python
scaler = StandardScaler()
df_final[['idade', 'renda', 'nota']] = scaler.fit_transform(df_final[['idade', 'renda', 'nota']])
```

üìå **Explica√ß√£o:** 

Este c√≥digo √© usado para **padronizar (normalizar)** as colunas num√©ricas de um DataFrame (`df_final`) usando a t√©cnica de **StandardScaler**, que faz parte da biblioteca **Scikit-learn**.

Cria-se  uma inst√¢ncia do objeto `StandardScaler`, chamado aqui de `scaler`. A padroniza√ß√£o ajusta os dados para que tenham escala semelhante, o que pode melhorar o desempenho dos modelos Machine Learning.

O m√©todo `.fit_transform()` realiza duas etapas em uma s√≥:
  1. **`.fit()`**: calcula a m√©dia e o desvio padr√£o das colunas `'idade'`, `'renda'` e `'nota'`.
  2. **`.transform()`**: aplica a padroniza√ß√£o nos dados com base nos valores calculados.

Assim, todos os valores s√£o reescalados para terem m√©dia zero e desvio padr√£o 1.

Finalmente substitui-se as colunas originais (`'idade'`, `'renda'`, `'nota'`) no DataFrame `df_final` pelos novos valores padronizados. Ou seja, ap√≥s essa linha:

- As colunas continuam existindo com os mesmos nomes.
- Mas agora cont√™m **valores padronizados** em vez dos originais.

üßæ **Resumindo**:

| Etapa | C√≥digo | O que faz |
|------|--------|-----------|
| 1 | `scaler = StandardScaler()` | Cria o objeto para padroniza√ß√£o |
| 2 | `scaler.fit_transform(...)` | Calcula e aplica a padroniza√ß√£o nas colunas num√©ricas |
| 3 | `df_final[[...]] = ...` | Atualiza o DataFrame com os dados padronizados |

üí° **Dica** Extra
- Se quiser ver os resultados, podemos usar:

```python
print(df_final[['idade', 'renda', 'nota']].describe())
```

Isso mostrar√° a m√©dia, desvio padr√£o e outras estat√≠sticas ‚Äî voc√™ ver√° que a m√©dia estar√° pr√≥xima de **0** e o desvio padr√£o pr√≥ximo de **1**.



## **4. Resumo do Fluxo de Trabalho**

1. **Carregar os dados** com Pandas.
2. **Explorar o dataset** com `.info()` e `.describe()`.
3. **Identificar valores ausentes** com `.isnull()`.
4. **Preencher valores ausentes** com t√©cnicas do Scikit-learn.
5. **Detectar e tratar outliers** com m√©todos estat√≠sticos.
6. **Transformar vari√°veis categ√≥ricas** com encoding.
7. **Padronizar dados num√©ricos** com Scaler.
8. **Preparar o dataset final** para modelagem.

---

## **6. Atividade Sugerida**

Use o dataset fornecido e aplique todos os passos vistos acima. Em seguida, treine um modelo simples de classifica√ß√£o (ex.: Regress√£o Log√≠stica) usando o Scikit-learn para prever a vari√°vel `categoria`.

Dica: Use `train_test_split` e `LogisticRegression`.


