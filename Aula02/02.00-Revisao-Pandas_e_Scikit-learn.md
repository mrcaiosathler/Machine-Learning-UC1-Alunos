# Bibliotecas Pandas e Scikit-learn

## **1. Apresentação das Bibliotecas**

### **Pandas**

O **Pandas** é uma biblioteca do Python usada principalmente para **manipulação e análise de dados**. Ele permite trabalhar com tabelas (chamadas de *DataFrames*), carregar dados de diferentes fontes (como CSV, Excel, SQL, etc.), fazer **filtros**, **agrupamentos**, **tratamento** de valores ausentes, entre outros.

### **Scikit-learn (sklearn)**

O **Scikit-learn** é uma biblioteca de **machine learning** que oferece ferramentas prontas para:

- **Pré-processamento** de dados

- **Modelagem estatística** 
  
  - Processo de usar técnicas e métodos estatísticos para **descrever, analisar e fazer previsões** com base em dados. 
  
  - O objetivo é **encontrar relações** entre variáveis e **entender padrões** nos dados, muitas vezes criando um modelo matemático que represente essas relações. 
  
  - Esse tipo de modelagem pode ser usada tanto para **explicar fenômenos** (como identificar fatores que influenciam uma determinada resposta) quanto para **prever resultados** futuros (como estimar vendas ou classificar categorias). 
  
  - A modelagem estatística está presente em diversas áreas, como economia, saúde, engenharia e ciências sociais, e serve como **base** para muitos algoritmos de **machine learning**.

- **Treinamento de modelos preditivos (regressão, classificação, clustering)**
  
  - É o processo de **ensinar um algoritmo** computacional a **reconhecer** padrões nos dados para fazer **previsões** sobre informações futuras ou desconhecidas. 
  
  - Esse treinamento pode ser dividido em **três grandes categorias**: 
  
    - **Regressão** : usada para **prever valores** numéricos contínuos, como prever o preço de uma casa; 
    
    - **Classificação** : que busca **identificar** a qual **categoria** ou classe um novo dado pertence, como determinar se um e-mail é spam ou não; 
    
    - **Clustering** : que **agrupa dados semelhantes** entre si sem a necessidade de rótulos prévios, sendo útil para **segmentação** de clientes ou **detecção de padrões** ocultos. 
    
  - Durante o treinamento, o modelo ajusta seus **parâmetros** com base em um conjunto de **dados conhecido** (dados de treino), buscando minimizar os erros nas previsões e generalizar bem para novos dados.

- **Avaliação de modelos**:
    
    - Processo de medir e **analisar o desempenho de um modelo** de machine learning após seu treinamento, para garantir que ele seja eficaz e **confiável ao fazer previsões** em dados novos e não vistos. 
    
    - Esse passo é essencial para **verificar** se o modelo **aprendeu** corretamente os padrões dos dados de treino e se consegue generalizar bem, ou seja, evitar problemas como **overfitting** (quando o modelo memoriza os dados de treino e falha nos novos dados) ou **underfitting** (quando o modelo não captura nenhum padrão relevante). 
    
    - Para isso, são utilizadas **métricas** específicas, como **acurácia**, **precisão**, **recall** e **F1-score** para classificação, e erro médio absoluto (MAE), erro quadrático médio (MSE) ou R² para regressão. Além disso, técnicas como validação cruzada ajudam a obter uma estimativa mais robusta do desempenho do modelo.


- **Validação cruzada**: 
    - Técnica estatística usada para **avaliar** e comparar o **desempenho** de modelos de machine learning, garantindo que eles sejam **capazes de generalizar** bem para dados novos e não vistos. 
    
    - Em vez de dividir os dados apenas em conjuntos de treino e teste uma única vez, a validação cruzada **divide os dados em várias partes** (ou "folds"), treinando e testando o modelo múltiplas vezes com diferentes combinações dessas partes. 
    
    - Isso permite obter **uma média mais precisa** do desempenho do modelo, reduzindo o impacto de variações aleatórias nos dados. 
    
    - A forma mais comum é a **k-fold cross-validation**, onde os dados são divididos em **k subconjuntos** iguais, e o modelo é treinado em k-1 deles e testado no restante, repetindo-se esse processo k vezes. 
    
    - Assim, a validação cruzada ajuda a evitar superajuste (overfitting ) e oferece uma visão mais confiável da eficácia do modelo.

- **Seleção de características** :

    - Também conhecida como **feature selection**, é o processo de **escolher o conjunto** mais relevante e significativo de variáveis (ou características) de um dataset para a construção de um modelo de machine learning. 
    
    - O objetivo é **identificar** as **colunas** que têm maior **poder preditivo**, descartando aquelas que não contribuem ou até prejudicam o desempenho do modelo. 
    
    - Isso **ajuda** a **reduzir** a **complexidade** do modelo, **evitar overfitting** (ajuste excessivo aos dados de treino), melhorar a velocidade de treinamento e aumentar a interpretabilidade do modelo. 
    
    - As **técnicas** de seleção de características podem ser divididas em:
        
        - **métodos filtros** - baseados em estatísticas dos dados;
        
        - **wrapper** - usam o próprio modelo para avaliar subconjuntos de features.
        
        - **embedded** - incorporam a seleção durante o treinamento do modelo, como em árvores de decisão ou regressão com regularização. 
        
    - Essa etapa é **fundamental no pré-processamento** de dados para construir modelos mais eficientes e confiáveis.

Essas duas bibliotecas são frequentemente utilizadas juntas em projetos de análise de dados e modelagem preditiva.

## **2. Objetivo da Aula**

Nesta aula, vamos revisar o processo de:

1. Carregar e explorar um dataset usando o **Pandas**
2. Identificar e tratar valores ausentes
3. Detectar e lidar com **outliers**
4. Trabalhar com variáveis **categóricas**
5. Preparar os dados para uso em algoritmos de **machine learning** com o **Scikit-learn**

Vamos usar um exemplo prático com o seguinte dataset:

| id  | idade | renda | cidade         | categoria | nota | feedback |
| --- | ----- | ----- | -------------- | --------- | ---- | -------- |
| 1   | 28    | 5000  | São Paulo      | A         | 7.5  | Bom      |
| 2   | NaN   | 3500  | Rio de Janeiro | B         | 6.2  | Regular  |
| 3   | 45    | NaN   | Belo Horizonte | A         | 9.0  | Bom      |
| 4   | 32    | 4200  | São Paulo      | B         | NaN  | Ruim     |

---

## **3. Passo a Passo com Exemplos Práticos**

Vamos começar importando as bibliotecas necessárias:

:snake: Codigo:

```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
```

:books: **Bibliotecas**

- **pandas** 
    - O Pandas é uma biblioteca essencial para **manipulação e análise** de dados em Python. Ele permite trabalhar com estruturas de dados como o DataFrame , que é semelhante a uma tabela do Excel ou um banco de dados relacional.
    
- **numpy**
    - O NumPy é uma biblioteca fundamental para **computação científica** em Python. Ela fornece suporte para **arrays multidimensionais** e **funções matemáticas** avançadas.

- **StandardScaler**
    -  **Padronizar** (ou normalizar) as variáveis numéricas, ou seja, transformar os valores para que tenham **média zero e desvio padrão igual a 1** .
    - Muitos algoritmos de machine learning (como KNN, Regressão Logística e SVM) funcionam melhor quando os dados estão na mesma escala.
    
```python
scaler = StandardScaler()
dados_padronizados = scaler.fit_transform(dados_numericos)
```
    
- **OneHotEncoder**
    - Converter **variáveis categóricas** (como "São Paulo", "Rio de Janeiro") **em números binários** (0s e 1s ) para que possam ser usadas em modelos de machine learning.
    - Modelos de ML não conseguem entender texto diretamente; eles precisam de dados numéricos.
    
```python
encoder = OneHotEncoder()
categorias_codificadas = encoder.fit_transform(df[['cidade']])
```
    
-  **SimpleImputer**

    - Tratar **valores ausentes** (também chamados de NaN ou null) nos dados.
    - Permite substituir valores faltantes pela **média**, **mediana**, **moda** ou um **valor constante**.
    - Algoritmos de machine learning geralmente **não aceitam dados faltantes**, então é necessário preenchê-los com algum valor razoável.
    
  ```python
  imputer = SimpleImputer(strategy='mean')  # preenche com a média
  df[['idade']] = imputer.fit_transform(df[['idade']])
  ```
Essas bibliotecas juntas formam uma base sólida para pré-processar dados antes de aplicar modelos de machine learning.

### **Passo 1: Carregar o Dataset**

Suponha que o dataset está salvo em um arquivo chamado `dados.csv`.

:snake: Código

```python
df = pd.read_csv('dados.csv')
print(df.head())
```
> 📌 **Explicação:** Estamos lendo o arquivo CSV e armazenando seus dados em um DataFrame chamado `df`. O método `.head()` mostra as primeiras linhas do dataset.


### **Passo 2: Explorar o Dataset**

#### Verificar tipos de dados e informações gerais:

```python
print(df.info())
```

Isso mostrará:

- Quantidade de linhas e colunas
- Tipo de cada coluna
- Número de valores não nulos

#### Estatísticas descritivas (para colunas numéricas):

```python
print(df.describe())
```

> 📌 **Explicação:** Isso ajuda a entender melhor a distribuição dos dados numéricos, como média, desvio padrão, valores mínimos e máximos.

---

### **Passo 3: Identificar Valores Ausentes**

Para identificar campos faltantes:

```python
print(df.isnull().sum())
```

> 📌 **Explicação:** O método `.isnull()` retorna True onde há valores ausentes. Usando `.sum()`, contamos quantos valores faltantes existem por coluna.

Exemplo de saída:

```
id          0
idade       1
renda       1
cidade      0
categoria   0
nota        1
feedback    0
```

### **Passo 4: Preencher Valores Ausentes (Imputação)**

Vamos preencher os valores ausentes nas colunas **numéricas**: `idade` e `nota`.

Usaremos a **média** para preencher esses campos:

```python
imputer_media = SimpleImputer(strategy='mean')
df[['idade', 'nota']] = imputer_media.fit_transform(df[['idade', 'nota']])
```

Para a coluna `renda`, poderíamos usar a mediana se houver muitos outliers:

```python
imputer_mediana = SimpleImputer(strategy='median')
df[['renda']] = imputer_mediana.fit_transform(df[['renda']])
```

#### :bulb: Uso do **SimpleImputer**

 O **SimpleImputer** é uma classe do Scikit-learn usada para **substituir** valores ausentes (**NaN**) em conjuntos de dados. Ele permite preencher os valores faltantes com diferentes estratégias.

Você pode escolher a estratégia de imputação usando o argumento `strategy` na criação do `SimpleImputer`. As opções mais comuns são:

| Estratégia (`strategy`) | Descrição |
|-------------------------|-----------|
| `'mean'`                | Substitui os valores ausentes pela **média** da coluna (apenas para dados numéricos). Útil quando os dados têm distribuição normal.|
| `'median'`              | Substitui os valores ausentes pela **mediana** da coluna (apenas para dados numéricos). Boa opção quando há **outliers**, pois a mediana é menos afetada por valores extremos. |
| `'most_frequent'`       | Substitui os valores ausentes pela **moda** (valor mais frequente) da coluna. Funciona para **dados numéricos e categóricos**. Ideal para **variáveis categóricas** ou numéricas com distribuição assimétrica. |
| `'constant'`            | Substitui os valores ausentes por um valor constante definido pelo usuário, através do parâmetro `fill_value`. Funciona para qualquer tipo de dado. Você define qual valor usar. Pode ser útil para indicar que um valor está ausente (ex: `-999`, `"desconhecido"`). |

🔧 **Exemplos de uso**

```python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
imputer = SimpleImputer(strategy='median')
imputer = SimpleImputer(strategy='most_frequent')
imputer = SimpleImputer(strategy='constant', fill_value=0)
imputer = SimpleImputer(strategy='constant', fill_value='Desconhecido')
```

📌 **Observações importantes:**

- O `SimpleImputer` **não modifica diretamente** o DataFrame. Você precisa aplicá-lo com `.fit_transform()` em seus dados.
- Para colunas categóricas, prefira `strategy='most_frequent'` ou `strategy='constant'`.
- A imputação deve ser feita **somente nos dados de treino**, para evitar **vazamento de dados** (*data leakage*) durante a modelagem preditiva.


:dart: Métodos suportados

Os objetos da classe `SimpleImputer` do **Scikit-learn** possuem vários métodos úteis para o processo de **imputação de valores ausentes**.

**🔧 1. `.fit(X)`**

- **Finalidade:**  
  Ajusta (treina) o imputador aos dados fornecidos.

- **O que ele faz?**  
  Calcula os valores a serem usados para preencher os dados faltantes, com base na estratégia escolhida (`mean`, `median`, `most_frequent`, `constant`).

- **Exemplo:**
  ```python
  imputer = SimpleImputer(strategy='mean')
  imputer.fit(X_train)
  ```

> O método `.fit()` **não modifica os dados**, apenas aprende os valores necessários (ex: média de cada coluna).


🔄 **2. `.transform(X)`**

- **Finalidade:**  
  Aplica a imputação nos dados, substituindo os valores ausentes pelos valores aprendidos no `.fit()`.

- **O que ele retorna?**  
  Um novo conjunto de dados (geralmente como um array NumPy), com os valores faltantes preenchidos.

- **Exemplo:**
  ```python
  X_treino_limpo = imputer.transform(X_train)
  ```

> Importante: Use `.transform()` somente **após** ter usado `.fit()`.


🚀 **3. `.fit_transform(X)`**

- **Finalidade:**  
  Combinação de `.fit()` + `.transform()` em uma única etapa.

- **Quando usar?**  
  Durante o treinamento do modelo, especialmente com os dados de treino.

- **Exemplo:**
  ```python
  X_treino_limpo = imputer.fit_transform(X_train)
  ```

> É mais eficiente e usado com frequência em pipelines e pré-processamento inicial.

📊 **4. `.get_feature_names_out()`**

- **Finalidade:**  
  Retorna os nomes das colunas após a transformação (útil quando usado em **pipelines** ou com outras transformações).

- **Exemplo:**
  ```python
  feature_names = imputer.get_feature_names_out()
  df_limpo = pd.DataFrame(X_treino_limpo, columns=feature_names)
  ```

> Este método é útil para manter o nome das colunas após a transformação


📝 **5. `.set_params()`**

- **Finalidade:**  
  Altera dinamicamente os parâmetros do imputador (como a estratégia ou valor constante).

- **Exemplo:**
  ```python
  imputer.set_params(strategy='median', fill_value=None)
  ```

> Útil se quiser reutilizar o mesmo objeto imputador com diferentes configurações.

📋 **6. `.statistics_`**

- **Finalidade:**  
  Atributo (não um método) que armazena os valores calculados durante o `.fit()` para cada coluna.

- **O que contém?**  
  Os valores usados para substituir os NaNs, como:
  - Média de cada coluna (se `strategy='mean'`)
  - Mediana (se `strategy='median'`)
  - Moda (se `strategy='most_frequent'`)
  - Valor constante (se `strategy='constant'`)

- **Exemplo:**
  ```python
  print(imputer.statistics_)
  ```

> Isso é útil para auditoria ou documentação: você pode ver exatamente quais valores foram usados na imputação.


✅ Resumo dos Métodos Mais Usados

| Método / Atributo        | Descrição |
|--------------------------|-----------|
| `.fit(X)`                | Aprende os valores para substituição (ex: média, mediana) |
| `.transform(X)`          | Aplica a substituição nos dados |
| `.fit_transform(X)`      | Faz `.fit()` e `.transform()` juntos |
| `.get_feature_names_out()` | Retorna os nomes das colunas após transformação |
| `.set_params()`          | Muda parâmetros do imputador |
| `.statistics_`           | Mostra os valores usados na substituição |

### **Passo 5: Identificar e Lidar com Outliers**

Vamos verificar possíveis **outliers** na coluna `idade`.

#### Visualização simples com boxplot:

```python
import matplotlib.pyplot as plt
plt.boxplot(df['idade'])
plt.title("Boxplot - Idade")
plt.show()
```

📌 **Explicando**:

Este código é usado para criar e exibir um gráfico de caixa (`boxplot`) da coluna **idade** de um DataFrame chamado df.

- A biblioteca **Matplotlib** , mais especificamente o módulo `pyplot`,  é usado para criar gráficos em 2D no Python. Usamos este módulo para visualizar dados graficamente, como histogramas, gráficos de linhas, dispersão e boxplots .

- **Boxplot** é um tipo de gráfico que mostra a distribuição dos dados com base nos quartis . Ele ajuda a identificar:
    - A mediana (valor central)
    - Os quartis (25%, 50%, 75%)
    - Valores extremos (outliers )
    - A caixa representa os valores entre o 1º quartil (**Q1**) e o 3º quartil (**Q3**).
    - A linha dentro da caixa é a **mediana** .
    - As "extremidades" das linhas ("**bigodes**") indicam os valores **mínimos** e **máximos** que não são outliers .
    - Pontos fora dessas extremidades são considerados outliers .
    
    ![Boxplot - Exemplo](img/g2.png)

- Matplotlib constrói o gráfico em memória primeiro, e plt.show() é o comando que realmente mostra o gráfico.

✅ **Resumo** 

| Linha | Código                            | O que faz                                               |
|-------|-----------------------------------|---------------------------------------------------------|
| 1     | `import matplotlib.pyplot as plt` | Importa a biblioteca de gráficos                        |
| 2     | `plt.boxplot(df['idade'])`        | Gera o gráfico de caixa com os dados da coluna 'idade'  |
| 3     | `plt.title("Boxplot - Idade")`    | Define o título do gráfico                              |
| 4     | `plt.show()`                      | Mostra o gráfico na tela                                |


#### Tratando os Outliers

Se encontrarmos outliers, podemos remover ou limitar os valores extremos:

```python
Q1 = df['idade'].quantile(0.25)
Q3 = df['idade'].quantile(0.75)
IQR = Q3 - Q1

limite_inferior = Q1 - 1.5 * IQR
limite_superior = Q3 + 1.5 * IQR

df = df[(df['idade'] >= limite_inferior) & (df['idade'] <= limite_superior)]
```

> 📌 **Explicação:** Acima, calculamos os quartis e removemos registros fora do intervalo considerado normal.

#### Reverificando com boxplot:

```python
import matplotlib.pyplot as plt
plt.boxplot(df['idade'])
plt.title("Boxplot - Idade")
plt.show()
```

### **Passo 6: Transformar Variáveis Categóricas**

As variáveis categóricas precisam ser convertidas em números para os algoritmos de ML entenderem.

#### Opção 1: One-Hot Encoding (para variáveis nominais)

```python
encoder = OneHotEncoder(sparse_output=False, drop='first')  
categorias_codificadas = encoder.fit_transform(df[['cidade', 'categoria', 'feedback']])

# Converter os resultados do encoding para um DataFrame
df_categorias = pd.DataFrame(categorias_codificadas, columns=encoder.get_feature_names_out(['cidade', 'categoria', 'feedback']))

# Resetar o índice do df original e concatenar com as novas colunas
df_final = pd.concat([df.reset_index(drop=True), df_categorias], axis=1)

# Remover as colunas categóricas originais (sem usar inplace)
df_final = df_final.drop(['cidade', 'categoria', 'feedback'], axis=1)
```

📌 **Explicação:** 

O `OneHotEncoding` cria uma nova coluna binária para cada categoria. Por exemplo, "São Paulo", "Rio de Janeiro" viram novas colunas com 0 ou 1.

O primeiro comando cria uma instância do **OneHotEncoder**, que será usada para transformar variáveis categóricas em colunas binárias (0 ou 1).

- **sparse_output=False**: retorna um **array NumPy** normal (não esparsa), mais fácil de trabalhar.

- **drop='first'**: remove a primeira categoria de cada variável para evitar multicolinearidade (muito útil em modelos como Regressão Linear).

O `fit_transform` ajusta o encoder às colunas categóricas (fit) e depois aplica a transformação (transform), gerando um array com as categorias convertidas em números. Ao final temos um array NumPy chamado **categorias_codificadas** com as novas colunas criadas pelo One-Hot Encoding.

O comando `pd.DataFrame` converte o array **categorias_codificadas** em um novo DataFrame chamado **df_categorias**.

Já o método `get_feature_names_out()` gera nomes descritivos para as novas colunas, como: **cidade_Sao Paulo**, **categoria_B**, **feedback_Ruim**.

O `pd.concat(..)` concatenando o novo DataFrame com o original

- **reset_index(drop=True)** : Reinicia os índices do DataFrame original (df) para garantir que ele combine corretamente com o novo (df_categorias).

- **pd.concat(..., axis=1)** : Une os dois DataFrames lado a lado (colunas do original + colunas codificadas).

Finalmente `df_final.drop(...)`, remove as colunas categóricas originais (`cidade`, `categoria`, `feedback`) do DataFrame final.

:bulb:  **Por que não usar `inplace=True`?**  
- Porque queremos seguir o estilo funcional, onde sempre atribuímos o resultado à variável. Isso ajuda a evitar efeitos colaterais e facilita o rastreamento de alterações no código.

💡 Visualizando o conteúdo do DataFrame final:

```python
print(df_final.head())
```

Isso mostrará as primeiras linhas do `df_final`, com as novas colunas criadas pelo **One-Hot Encoder**.

### **Passo 7: Normalizar ou Padronizar Dados Numéricos**

Alguns algoritmos de ML funcionam melhor com dados padronizados (média zero e desvio padrão 1).

```python
scaler = StandardScaler()
df_final[['idade', 'renda', 'nota']] = scaler.fit_transform(df_final[['idade', 'renda', 'nota']])
```

📌 **Explicação:** 

Este código é usado para **padronizar (normalizar)** as colunas numéricas de um DataFrame (`df_final`) usando a técnica de **StandardScaler**, que faz parte da biblioteca **Scikit-learn**.

Cria-se  uma instância do objeto `StandardScaler`, chamado aqui de `scaler`. A padronização ajusta os dados para que tenham escala semelhante, o que pode melhorar o desempenho dos modelos Machine Learning.

O método `.fit_transform()` realiza duas etapas em uma só:
  1. **`.fit()`**: calcula a média e o desvio padrão das colunas `'idade'`, `'renda'` e `'nota'`.
  2. **`.transform()`**: aplica a padronização nos dados com base nos valores calculados.

Assim, todos os valores são reescalados para terem média zero e desvio padrão 1.

Finalmente substitui-se as colunas originais (`'idade'`, `'renda'`, `'nota'`) no DataFrame `df_final` pelos novos valores padronizados. Ou seja, após essa linha:

- As colunas continuam existindo com os mesmos nomes.
- Mas agora contêm **valores padronizados** em vez dos originais.

🧾 **Resumindo**:

| Etapa | Código | O que faz |
|------|--------|-----------|
| 1 | `scaler = StandardScaler()` | Cria o objeto para padronização |
| 2 | `scaler.fit_transform(...)` | Calcula e aplica a padronização nas colunas numéricas |
| 3 | `df_final[[...]] = ...` | Atualiza o DataFrame com os dados padronizados |

💡 **Dica** Extra
- Se quiser ver os resultados, podemos usar:

```python
print(df_final[['idade', 'renda', 'nota']].describe())
```

Isso mostrará a média, desvio padrão e outras estatísticas — você verá que a média estará próxima de **0** e o desvio padrão próximo de **1**.



## **4. Resumo do Fluxo de Trabalho**

1. **Carregar os dados** com Pandas.
2. **Explorar o dataset** com `.info()` e `.describe()`.
3. **Identificar valores ausentes** com `.isnull()`.
4. **Preencher valores ausentes** com técnicas do Scikit-learn.
5. **Detectar e tratar outliers** com métodos estatísticos.
6. **Transformar variáveis categóricas** com encoding.
7. **Padronizar dados numéricos** com Scaler.
8. **Preparar o dataset final** para modelagem.

---

## **6. Atividade Sugerida**

Use o dataset fornecido e aplique todos os passos vistos acima. Em seguida, treine um modelo simples de classificação (ex.: Regressão Logística) usando o Scikit-learn para prever a variável `categoria`.

Dica: Use `train_test_split` e `LogisticRegression`.


